# Principled Data Processing in Python 

Pierson Browne   
University of Waterloo 

One of my all-time favourite films is Danny Boyle’s oft-overlooked Sunshine (2007). It tells a fictional story about how 8 of Earth’s leading astronauts and scientists make a series of well-intentioned, expertise-based decisions which, in aggregate, doom them and their mission. In other words, the movie tells us that a large number of good micro-scale decisions can combine to create one terrible macro-scale outcome.

When I think about the pitfalls I’ve encountered whilst working on collaborative data processing efforts within the academy, I’m often reminded of Sunshine: groups of intelligent scholars are brought together and tasked with coaxing publishable insights from a corpus of raw input data. Such projects are frequently – though unintentionally -- developed following tumultuous, organic logics. Each member is tasked with composing a script or module capable of pushing the pipeline closer to the predefined end goal. Such tasks are frequently allotted along internal perceptions of relative expertise: even in cases where a team commences a project with members who share a relatively homogenous skillset, I’ve found that individual members autonomously seek out and address gaps in the team’s collective wisdom. This tendency towards utilizing (or developing) expertise is useful for getting things done efficiently, but virtually guarantees that no one member of a team will understand how the finished project works or how its constituent pieces fit together. The result is a ‘black box’ which performs one task very well, but can’t be easily understood, changed, fixed, or applied to cases other than the one it was built for.

Blaming individuals isn’t productive: all of the developers involved in creating these Kafkaesque labyrinths of code and data are almost certainly making smart, well-informed decisions at every step in the process. The ghastly state of the deliverable is a result of many good decisions made without the benefit of a set of organizing principles. Principled Data Processing – as described by Dr. Patrick Ball – is a framework designed to help avoid these pitfalls.  

In pushing for adopting the precepts of Principled Data Processing, one of the problems I wanted to address was ease of adoption. Dr. Ball’s system is easily implementable using nothing more than discipline and a small suite of tools which come pre-installed with most of the common Linux distributions. Attempting to implement Principled Data Processing as a set of principles and guidelines, however, created almost as many problems as it solved. The principles guiding Dr. Ball’s system were open to individual interpretation, and our team’s implementation of them – and willingness to adhere to -- them varied wildly from individual to individual.  

When I set about designing the [PDPP package](https://github.com/UWNETLAB/pdpp), I was determined to make Principled Data Processing as accessible as possible without sacrificing any of the benefits the system offered. The result is a cross-platform command line interface that – with luck – reduces the amount of upkeep and busywork needed to properly create and maintain an analytical data processing pipeline, while ensuring that the end result is transparent, reproducible, and can be understood by everyone involved. 
