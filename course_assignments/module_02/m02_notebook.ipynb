{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=\"gray\">DOING COMPUTATIONAL SOCIAL SCIENCE<br>MODULE 2 <strong>PROBLEM SETS</strong></font>\n",
    "\n",
    "# <font color=\"#49699E\" size=40>MODULE 2 </font>\n",
    "\n",
    "\n",
    "# What You Need to Know Before Getting Started\n",
    "\n",
    "- **Every notebook assignment has an accompanying quiz**. Your work in each notebook assignment will serve as the basis for your quiz answers.\n",
    "- **You can consult any resources you want when completing these exercises and problems**. Just as it is in the \"real world:\" if you can't figure out how to do something, look it up. My recommendation is that you check the relevant parts of the assigned reading or search for inspiration on [https://stackoverflow.com](https://stackoverflow.com).\n",
    "- **Each problem is worth 1 point**. All problems are equally weighted.\n",
    "- **The information you need for each problem set is provided in the blue and green cells.** General instructions / the problem set preamble are in the blue cells, and instructions for specific problems are in the green cells. **You have to execute all of the code in the problem set, but you are only responsible for entering code into the code cells that immediately follow a green cell**. You will also recognize those cells because they will be incomplete. You need to replace each blank `▰▰#▰▰` with the code that will make the cell execute properly (where # is a sequentially-increasing integer, one for each blank).\n",
    "- Most modules will contain at least one question that requires you to load data from disk; **it is up to you to locate the data, place it in an appropriate directory on your local machine, and replace any instances of the `PATH_TO_DATA` variable with a path to the directory containing the relevant data**.\n",
    "- **The comments in the problem cells contain clues indicating what the following line of code is supposed to do.** Use these comments as a guide when filling in the blanks. \n",
    "- **You can ask for help**. If you run into problems, you can reach out to John (john.mclevey@uwaterloo.ca) or Pierson (pbrowne@uwaterloo.ca) for help. You can ask a friend for help if you like, regardless of whether they are enrolled in the course.\n",
    "\n",
    "Finally, remember that you do not need to \"master\" this content before moving on to other course materials, as what is introduced here is reinforced throughout the rest of the course. You will have plenty of time to practice and cement your new knowledge and skills.\n",
    "<div class='alert alert-block alert-danger'>As you complete this assignment, you may encounter variables that can be assigned a wide variety of different names. Rather than forcing you to employ a particular convention, we leave the naming of these variables up to you. During the quiz, use the 'USER_DEFINED' option to fill in any blank that you assigned an arbitrary name to.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 01:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "The cell below contains partially complete code to read a text file into memory. Replace each blank in the code block to successfully read in the file and print the contents to screen.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_TO_DATA/'countries.txt', 'r', encoding='utf-8') as file:\n",
    "    countries = [country.strip('\\n') for country in file]\n",
    "\n",
    "▰▰0▰▰(countries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 02:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "The cell below iterates over each country name in our list using a for loop and appends an all caps version of the country name to a new list. Complete the code cell below to rewrite the for loop using list comprehension and then confirm that the two lists are identical.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_fr = []  # an empty list to contain the results of our for loop\n",
    "for country in countries:\n",
    "    upper_fr.append(country.upper())\n",
    "\n",
    "upper_lc = [▰▰0▰▰.upper() for ▰▰1▰▰ in ▰▰2▰▰]\n",
    "\n",
    "# confirm they are the same\n",
    "upper_fr == upper_lc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 03:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "The cell below iterates over the list of countries and creates a new list containing the number of characters in each country name. Complete the code to translate this list comprehension into a for loop and confirm that the results are identical.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_lc = [len(c) for c in countries]  # list comprehension\n",
    "\n",
    "# for loop\n",
    "len_fr = []\n",
    "for ▰▰0▰▰ in ▰▰1▰▰:\n",
    "    ▰▰2▰▰.append(len(▰▰3▰▰))\n",
    "\n",
    "len_lc == len_fr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 04:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "The cell below reads the content of a second text file containing the abbreviated names for each of the countries in the initial list. It then uses the zip() function to associate the information in both lists based on index positions, and adds the information to a dictionary where the key is the abbreviated name and the value is the full name. Complete the code cell.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_TO_DATA/'cids.txt', 'r', encoding='utf-8') as f:\n",
    "    cids = [line.strip('\\n') for line in f]\n",
    "    \n",
    "names_and_ids = {}    \n",
    "\n",
    "for ▰▰0▰▰ in zip(▰▰1▰▰, ▰▰2▰▰):\n",
    "    names_and_ids[▰▰3▰▰[0]] = ▰▰4▰▰[1]\n",
    "\n",
    "names_and_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 05:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "The code below reads a Python dictionary into memory using a package called pickle. The keys in this dictionary are country names, and the values are lists containing the following three pieces of information:<br>\n",
    "<ol>\n",
    "<li>The two letter abbreviation of the country name\n",
    "<li>A core civil society index (CCSI) constructed from a number of low-level indicators, including the extent to which major civil society organizations (CSOs) are consulted by policymakers; how large the involvement of people in CSOs is; whether women are prevented from participating; and whether legislative candidate nomination within party organization is highly decentralized or made through party primaries? The index values range from 0 to 1.\n",
    "<li>A free association index that measures the extent to which parties, including opposition parties, allowed to form and to participate in elections, and the extent to which civil society organizations are able to form and to operate freely. Scores range from 0 to 1.\n",
    "</ol> \n",
    "In the code block below, print the CCSI (Core Civil Society Index) for Brazil (in 2019).\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(PATH_TO_DATA/'country_dicts.pkl', 'rb') as f:\n",
    "    country_data = pickle.load(f)\n",
    "\n",
    "brazil = country_data[▰▰0▰▰][▰▰1▰▰]\n",
    "print(f'{brazil}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 06:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "The cell below accepts the names of two countries, retrieves their CCSI values from the country_data dict, and then returns the name and score for the country with the higher value in a formatted string. Use the function to compare Japan and South Korea. Complete the cell.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_countries(country_a, country_b, country_data):\n",
    "    if ▰▰0▰▰[▰▰1▰▰][1] > ▰▰2▰▰[▰▰3▰▰][1]:\n",
    "        print(f'{▰▰4▰▰} ({country_data[▰▰5▰▰][1]})')\n",
    "    else:\n",
    "        return f'{▰▰6▰▰} ({country_data[▰▰7▰▰][1]}) had the higher CCSI value in 2019.'\n",
    "\n",
    "compare_countries(▰▰8▰▰, ▰▰9▰▰, ▰▰10▰▰)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 07:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "The cell below defines a function that accepts two country names as arguments, gets their CCSI values (Core Civil Society Index) from our country_data dict, and then returns to the difference between the two countries by subtracting the CCSI of the second country from the CCSI of the first. Use the function to compare the CCSI values for Canada and Argentina. Complete the cell.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ccsi(▰▰0▰▰, country_b, country_data):\n",
    "    diff = country_data[▰▰1▰▰][▰▰2▰▰] - country_data[country_b][▰▰3▰▰]\n",
    "    return diff\n",
    "\n",
    "compare_ccsi(▰▰4▰▰, ▰▰5▰▰, ▰▰6▰▰)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 08:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "The cell below defines a function that accepts a list of country names (well, actually any string) as an argument. It iterates over each country in the list and retrieves the three letter country abbreviation from the list of dictionaries we just used. It outputs a list of tuples, where the first element of the tuple is the country name and the second is the abbreviated name. The function uses a bit of try / except logic to handle potential errors (e.g. encountering a string that doesn't match any of the keys in the country dictionaries). Complete the cell.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorganize_data(list_of_countries, country_data):\n",
    "    results = []\n",
    "    problems = []\n",
    "    for ▰▰0▰▰ in list_of_countries:\n",
    "        try:\n",
    "            restructured = tuple([country, ▰▰1▰▰[country][0]])\n",
    "            results.append(▰▰2▰▰)\n",
    "        except:\n",
    "            ▰▰3▰▰.append(country)\n",
    "            \n",
    "    return results, problems\n",
    "\n",
    "reorganized, problems = reorganize_data(['Canada', 'Brazil', 'Norway', 'Brazanada'], country_data)\n",
    "print(f'Sucessfully reorganized: {[country[▰▰4▰▰] for country in reorganized]}')\n",
    "print(f'I have never heard of: {[▰▰5▰▰ for ▰▰6▰▰ in problems]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 09:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "The file sanders_oped.txt contains the text of an op-ed that Bernie Sanders published in The Guardian on Wednesday January 20th, 2021, the day that Joe Biden and Kamala Harris were sworn in as President and Vice-President of the United States. The code cell below reads in the text file, strips out line breaks ('\n",
    "'), and ignores lines that don't contain any text. Complete the missing code in the function get_sentences(), which attempts to convert the lists of paragraphs to a list of individual sentences by splitting on periods.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_TO_DATA/'sanders_oped.txt', 'r', encoding='utf-8') as file:\n",
    "    oped = [line.strip('\\n') for line in file if len(line) > 1]\n",
    "\n",
    "\n",
    "def get_sentences(text):\n",
    "    results = []\n",
    "    for paragraph in text:\n",
    "        sentences = ▰▰0▰▰.split('.')\n",
    "        for ▰▰1▰▰ in sentences:\n",
    "            if len(▰▰2▰▰) > 1:\n",
    "                results.append(▰▰3▰▰)\n",
    "    return results\n",
    "\n",
    "\n",
    "sentences = get_sentences(oped)\n",
    "sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Complete the function below, which will accept a list of strings and return a new list containing the length of each string (i.e. the number of words in the string). Count the number of words in each sentence of Sander's oped by passing the oped list of strings to your new function and then confirm that the list of numbers and the list of strings are the same length.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senlens(▰▰0▰▰):\n",
    "    result = [▰▰1▰▰(sentence) ▰▰2▰▰ sentence ▰▰3▰▰ oped]\n",
    "    return ▰▰4▰▰\n",
    "\n",
    "\n",
    "lens = senlens(oped)\n",
    "print(lens)\n",
    "len(▰▰5▰▰) == len(oped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "The cell below contains a for loop that iterates over the sentences in Sander's oped and creates two new lists: one for sentences that contain the word \"Republican,\" the other for sentences that contain the word \"Democrat.\" Complete the code and then print the lengths of each.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "democrat = []\n",
    "republican = []\n",
    "\n",
    "\n",
    "for sentence in sentences:\n",
    "    if \"Democrat\" in ▰▰0▰▰:\n",
    "        ▰▰1▰▰.append(▰▰2▰▰)\n",
    "    if \"Republican\" in ▰▰3▰▰:\n",
    "        ▰▰4▰▰.append(▰▰5▰▰)\n",
    "\n",
    "print(f'{len(▰▰6▰▰)} sentence(s) contain(s) the word Democrat and {len(▰▰7▰▰)} sentence(s) contain(s) the word Republican.')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "The cell below iterates over the sentences in Sanders' oped and prints the sentence to screen if it contains a word of interest. Complete the code cell so that it searches for the word 'need'.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search = 'need'\n",
    "\n",
    "▰▰0▰▰ sentence ▰▰1▰▰ sentences:\n",
    "    ▰▰2▰▰ ▰▰3▰▰ ▰▰4▰▰ sentence:\n",
    "        print(▰▰5▰▰)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Wading into endlessly nested JSON structures and sorting through text that's full of HTML tags can be a bit daunting. Thankfully, the well-structured nature of API responses means that more straightforward numeric data is usually easily accessed. Before starting, make sure your `cred.py` is properly setup like the example in the chapter.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "In this exercise, you will search the Guardian API for the term \"morrison\" between the dates of 2019-05-18 and 2019-05-19 - the day Australian Prime Minister Scott Morrison won an election and the day after. Using a loop, search for the term in each of the three production offices ('aus', 'uk', and 'us') and store each result in a list. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from IPython.core.display import display, HTML\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from pprint import pprint\n",
    "\n",
    "import cred\n",
    "\n",
    "# API key provided by the Guardian\n",
    "GUARDIAN_KEY = cred.GUARDIAN_KEY\n",
    "\n",
    "# Initialize Constants:\n",
    "API_ENDPOINT = 'http://content.guardianapis.com/search' \n",
    "office_list = ['aus','uk','us']\n",
    "\n",
    "# Set up the request parameters, including authorization\n",
    "PARAMS = {'api-key': ▰▰0▰▰,                \n",
    "             'from-date': '2019-05-18',\n",
    "             'to-date': '2019-05-19',\n",
    "             'q': 'morrison'}\n",
    "\n",
    "# Initialize list for storing results from iteration\n",
    "morrison_list = []\n",
    "\n",
    "# Iterate over each of the offices in 'office list'\n",
    "▰▰1▰▰ office ▰▰2▰▰ office_list:\n",
    "    \n",
    "    PARAMS['production-office'] = office      # set the search filter on each pass of the loop\n",
    "    response = requests.▰▰3▰▰(▰▰4▰▰, params=PARAMS)    # send the query to the Guardian API\n",
    "    response_dict = response.json()['response']         # keep the relevant component of the response\n",
    "    \n",
    "    total_articles = response_dict['total']        # access the metafield that indicates the total number of responses\n",
    "    morrison_list.▰▰5▰▰(▰▰6▰▰)           # add the resulting data to the list of results\n",
    "    \n",
    "    print(office + ': ' + str(total_articles))        # print results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Next we'll grab another search term - this time 'trump' - for the same time period and the same set of three production offices. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Query the Guardian API for 'trump' and store the results in a list much like last time. Create a pandas dataframe out of all three lists, giving each column a reasonably descriptive name. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list for storing results from iteration\n",
    "trump_list = []\n",
    "\n",
    "#Modify the search term\n",
    "PARAMS['q'] = 'trump'                  \n",
    "\n",
    "# Iterate over each of the offices in 'office list'\n",
    "▰▰0▰▰ office ▰▰1▰▰ ▰▰2▰▰:\n",
    "    \n",
    "    PARAMS['production-office'] = office # set the search filter on each pass of the loop\n",
    "    \n",
    "    response = ▰▰3▰▰.get(API_ENDPOINT, params=▰▰4▰▰)  # send the query to the Guardian API\n",
    "    response_dict = response.▰▰5▰▰()[▰▰6▰▰] # keep the relevant component of the response\n",
    "    \n",
    "    total_articles = response_dict['total'] # access the metafield that indicates the total number of responses\n",
    "    trump_list.append(total_articles)       # add the resulting data to the list of results\n",
    "\n",
    "# Initialize an empty dataframe and create columns from the lists\n",
    "df = pd.▰▰7▰▰()\n",
    "df['office'], df['morrison'], df['trump'] = office_list, morrison_list, trump_list       \n",
    "\n",
    "# See the whole (small) dataframe\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 15:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Let's take a look at what kind of stories from the UK office were mentioning Trump in that time period.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Reconfigure the appropriate `PARAMS` dictionary entries to carry out the search, adding 'headline' to the request. Retrieve the headlines from each article returned in the response, store them in a list, and take a look at the topics suggested by the headlines! \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list for storing results from iteration\n",
    "trump_uk_headlines = []\n",
    "\n",
    "# Change some of the request parameters again\n",
    "PARAMS['q'] = 'trump'                          \n",
    "PARAMS['production-office'] = 'uk'\n",
    "PARAMS['show-fields'] = 'headline'\n",
    "\n",
    "response = ▰▰0▰▰.▰▰1▰▰(▰▰2▰▰, params=▰▰3▰▰)  # send the query to the Guardian API\n",
    "response_dict = response.▰▰4▰▰()[▰▰5▰▰]           # keep the relevant component of the response\n",
    "\n",
    "# Iterate over each of the responses\n",
    "▰▰6▰▰ resp ▰▰7▰▰ response_dict[▰▰8▰▰]:\n",
    "    headline = resp['fields']['headline']        # process the new result field\n",
    "    trump_uk_headlines.▰▰9▰▰(headline)          # add the resulting data to the list of results\n",
    "    \n",
    "# View the list you just finished making\n",
    "pprint(trump_uk_headlines) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Word counts are also often at least a byproduct of many API processes, so they will often be available. Let's see if there was much difference in the average word count of articles mentioning Trump, across all production offices, compared to those mentioning Morrison.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Modify the relevant entries in `PARAMS`, retrieving the wordcount for each article in the response text. Results should be stored in a list for each politician to calculate and print the average word counts.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_04\n",
    "\n",
    "# Initialize lists for storing results from iteration\n",
    "morrison_counts = []\n",
    "trump_counts = []\n",
    "\n",
    "# Need to request a new field and clear the office filter\n",
    "PARAMS['q'] = 'morrison'\n",
    "PARAMS['production-office'] = None\n",
    "PARAMS['show-fields'] = 'wordcount'             \n",
    "\n",
    "response = ▰▰0▰▰.▰▰1▰▰(▰▰2▰▰, params=▰▰3▰▰)   # send the query to the Guardian API\n",
    "response_dict = response.json()[▰▰4▰▰]            # keep the relevant component of the response \n",
    "\n",
    "# Fill the list of words counts for the first search\n",
    "for resp in response_dict['results']:\n",
    "    wordcount = resp['fields']['wordcount']       # retrieve the word count\n",
    "    morrison_counts.▰▰5▰▰(int(wordcount))        # int() is needed because the API returns strings\n",
    "\n",
    "PARAMS['q'] = 'trump'\n",
    "response = ▰▰6▰▰.▰▰7▰▰(▰▰8▰▰, params=▰▰9▰▰)   # send the query to the Guardian API\n",
    "response_dict = response.json()[▰▰10▰▰]            # keep the relevant component of the response\n",
    "\n",
    "# Fill the list of words counts for the second search\n",
    "for resp in ▰▰11▰▰['results']:\n",
    "    wordcount = ▰▰12▰▰['fields']['wordcount']\n",
    "    trump_counts.▰▰13▰▰(int(wordcount))\n",
    "\n",
    "# Calculate the average of the word counts\n",
    "morrison_avg = np.mean(morrison_counts)        \n",
    "print(morrison_avg)\n",
    "trump_avg = np.mean(trump_counts)\n",
    "print(trump_avg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 17:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "For this exercise, we're going to use the power of scraping to delve into the results of the 2019 Canadian Federal Election! Just as every journey begins with a single step, we're going to start out with some basics. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Use the URL for Wikipedia's riding-by-riding election results to retrieve the web page and then check to see the if the result from the server was ok.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store our website address in the 'url' variable\n",
    "url = \"https://en.wikipedia.org/wiki/Results_of_the_2019_Canadian_federal_election\"\n",
    "\n",
    "# Retrieve the website\n",
    "r = ▰▰0▰▰.▰▰1▰▰(▰▰2▰▰)\n",
    "\n",
    "# Query as to whether or not our request was 'ok' and display result\n",
    "print(r.▰▰3▰▰)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 18:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Now that we have the HTML in hand, let's use BeautifulSoup to process the website and get its on-screen title (the text that's immediately above the 'From Wikipedia, the free encyclopedia'). Since the on-screen title differs somewhat from the tab title that we retreived in the chapter on scraping, you might need to do a little digging in the site's HTML to figure out where it's stored. (Hint: you can find it in the 'body' of the article, and it is a type of heading)\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Process the website using BeautifulSoup and find the on-screen title of the website we retrieved. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BeautifulSoup to create an HTML DOM\n",
    "soup = ▰▰0▰▰(r.▰▰1▰▰, 'lxml')\n",
    "\n",
    "# Use the soup object to find the text of the web page's on-screen title\n",
    "on_screen_title = soup.▰▰2▰▰(\"h1\")[0].text\n",
    "\n",
    "# Display Result\n",
    "print(on_screen_title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 19:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "If you scroll around on the webpage we're scraping information from, you might notice that the centrepiece of the page is a large table entitled 'Results by riding - 2019 Canadian federal election'. It contains a whole lot of data; it would be great to have access to all of it in an organized fashion! Fortunately, we can easily find tables in this article by searching for objects with the 'table' tag. Unfortunately, there are 27 such tables in the article, and we can't be certain of the order they appear in BeautifulSoup's 'findAll' results. There are many ways to programmatically ensure that you've got the correct table. In this question, we're going to take advantage of the fact that we know the name of the table we're looking for. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Iterate through the HTML tables in the web page to locate the \"Results by riding\" table.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the tables in the web page\n",
    "list_of_tables = soup.▰▰0▰▰(\"table\")\n",
    "\n",
    "# Initialize the variable we'll use to store the index\n",
    "result_table_index = None\n",
    "\n",
    "# Iterate over each table in the wikipedia article\n",
    "for ii, table in ▰▰1▰▰(list_of_tables):\n",
    "    first_row = table.▰▰2▰▰('tr')[0].▰▰3▰▰ # Get the first row of the table\n",
    "    if \"Results by riding - 2019 Canadian federal election\" in ▰▰4▰▰:\n",
    "        result_table_index = ▰▰5▰▰  # If we get a match, we've found our table!\n",
    "\n",
    "# Display index of the result table\n",
    "print(result_table_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 20:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Now that we have the index of the table we want, we can easily retrieve it from our `list_of_tables`. All of the juicy, riveting details of the election are almost within reach, but before we can grasp them, we're going to need to know a bit more about how the data is organized. Since this is an HTML table, you can think of it as being built from a large number of 'rows' in the table. Take some time to play around with the table in your browser using development tools. We're interested in figuring out what HTML tag is use to denote a single row in the table (take, for example, Calgary Nose Hill - what's the tag that is used to identify its entire row in the table? Make sure you're inside the table's 'tbody' tag, and not its 'thead' tag).  \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Find the HTML tag used to designate a single row of data in this table.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_08\n",
    "\n",
    "# Store the two-letter HTML tag you found as part of your investigation (in string format)\n",
    "row_tag = \"▰▰0▰▰\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 21:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Now that we have access to the rows of the table, we can look through them to find all of the data corresponding to any given riding! We might as well keep things close to 'home'; in the following code cell, we're going to produce a list of all the rows in the table's body and then locate the row corresponding to the 'Waterloo' riding. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Create a list of all of the rows in the 'Results by riding' table, and then locate the 'Waterloo' riding's row. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the \"Results by riding\" table from the list of tables\n",
    "results_table = ▰▰0▰▰[▰▰1▰▰]\n",
    "\n",
    "# Create a list of all rows in the table by searching for the row tag you found\n",
    "all_rows = results_table.▰▰2▰▰(row_tag)\n",
    "\n",
    "# Iterate through list of rows to find 'Waterloo' row.\n",
    "for i, row in enumerate(all_rows):\n",
    "    if \"Waterloo\" ▰▰3▰▰ row.▰▰4▰▰:\n",
    "        waterloo_row = row\n",
    "        \n",
    "# Process the results into a human-readable form:\n",
    "waterloo_text = [r for r in waterloo_row.text.▰▰5▰▰(\"\\n\") if r]\n",
    "\n",
    "print(waterloo_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 22:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "If everything went well, we should now have access to the 'waterloo_text' variable, which is a list of 18 strings we created from the Riding of Waterloo's row in the 'Results by riding' table. Fortunately for us, all of the other data rows should follow the exact same pattern. We can use this inter-row regularity to create a Pandas dataframe that should closely match the table in the wikipedia article. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Remove the first 5 rows of the 'all_rows' variable (index positions 0 to 4). Then, populate the pandas dataframe (we've filled in the column names already) with the rows from the table you scraped. Finally, find the number of ridings from each province and territory.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove first 5 items from the 'all_rows' list\n",
    "all_but_five_rows = ▰▰0▰▰[5 ▰▰1▰▰ ▰▰2▰▰]\n",
    "\n",
    "# Initialize a list of the columns from the Wikipedia table\n",
    "riding_cols = [\n",
    "    'riding',\n",
    "    'province_or_territory',\n",
    "    '2015_winning_party',\n",
    "    '2019_winning_party',\n",
    "    'votes',\n",
    "    'share',\n",
    "    'margin_num',\n",
    "    'margin_pct',\n",
    "    'turnout',\n",
    "    'liberal',\n",
    "    'conservative',\n",
    "    'ndp',\n",
    "    'bloc',\n",
    "    'green',\n",
    "    'ppc',\n",
    "    'independent',\n",
    "    'other',\n",
    "    'total',\n",
    "    'riding_url',\n",
    "]\n",
    "\n",
    "# Initialize dataframe\n",
    "riding_df = ▰▰3▰▰.▰▰4▰▰(▰▰5▰▰=riding_cols)\n",
    "\n",
    "# Populate dataframe with rows\n",
    "for row in all_but_five_rows:\n",
    "    row_text = [r for r in row.text.replace(',','').split(\"\\n\") if r]\n",
    "    row_text.append(row.find('a', href=True)['href'])\n",
    "\n",
    "    while len(row_text) < 19:\n",
    "        row_text.insert(-2, 0) # This fixes 3 broken rows\n",
    "\n",
    "    df_row = pd.Series(row_text, index=riding_df.columns)\n",
    "    riding_df = riding_df.append(df_row, ignore_index=True)\n",
    "    \n",
    "\n",
    "# Count the number of ridings from each province and territory\n",
    "riding_counts = ▰▰6▰▰[▰▰7▰▰].▰▰8▰▰()\n",
    "\n",
    "riding_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 23:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "If you're not a student of Canadian politics, it might surprise you to learn that in the 2019 election, the incumbent Liberal Party of Canada (Lib) recieved fewer votes than the Conservative Party of Canada (Con), and yet the Liberals won more seats than the Conservatives and formed government. This happened because each riding in the Canadian electoral system runs according to a winner-takes-all logic, where 100% of the rewards go to the cadidate who finished in first place, even if they only beat the person in second place by a single vote. This means that having a large margin of victory in a single riding is not desirable - presumably, that represents time, resources, and effort that could have been more efficiently allocated. In this question, we're going to see if we can use the data we scraped to help shed some light on why this strange state of affairs came to pass.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Examine the twenty ridings with the highest margin of victory. Then, examine the twenty ridings with the lowest margin of victory. The margin of victory is contained in the 'margin_num' and 'margin_pct' columns.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the margin_num column to a numeric column\n",
    "riding_df['margin_num'] = pd.to_numeric(riding_df['margin_num'])\n",
    "\n",
    "# Sort the entire dataframe by the value of the margin number, ascending\n",
    "margin_df_ascending  = riding_df.▰▰0▰▰(['margin_num'], ascending=▰▰1▰▰)\n",
    "\n",
    "# display the 20 ridings with the largest margin of victory\n",
    "display(HTML('<div class=\"alert alert-block alert-info\">Ridings with highest margin of victory</div>'))\n",
    "display(margin_df_ascending.▰▰2▰▰(▰▰3▰▰))\n",
    "\n",
    "# display the 20 ridings with the lowest margin of victory\n",
    "display(HTML('<div class=\"alert alert-block alert-danger\">Ridings with lowest margin of victory</div>'))\n",
    "display(margin_df_ascending.▰▰4▰▰(▰▰5▰▰))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 24:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "For this final problem, we're going to harness the information we've already gathered to create a scraper that's capable of semi-autonomously traversing a (vanishingly small) proportion of Wikipedia's pages. Specifically, we're going to take advantage of the fact that each of the riding entries that we scraped contains a link to Wikipedia's article on that riding. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Create a scraper capable of retrieving the 'Date created' date for each of the 20 ridings with the lowest margin of victory. Average all of these dates and round them to the nearest full year. Make certain that your scraper is capable of handling riding articles that do not contain a 'First Contested' date.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "link_base = \"https://en.wikipedia.org/\"\n",
    "year_created_list = []\n",
    "\n",
    "# create list of first 20 rows:\n",
    "first_twenty_rows = list(margin_df_ascending.iterrows())[0 ▰▰0▰▰ 20]\n",
    "\n",
    "# retrieve 'District Created' for each link in list\n",
    "for i, row in first_twenty_rows:\n",
    "    sleep(0.5)\n",
    "\n",
    "    r = ▰▰1▰▰.▰▰2▰▰(link_base + row['riding_url']) # Send request to Wikipedia\n",
    "    soup = ▰▰3▰▰(r.▰▰4▰▰, 'lxml') # Process using beautiful soup\n",
    "    for row in soup.findAll('tr'): # Iterate over our 'soup' DOM\n",
    "        if 'District created' in row.text: # If we find a match, add the value.\n",
    "            year_created_list.append(int(row.find('td').text))\n",
    "            \n",
    "# Find the average year of creation and round it to the nearest full year\n",
    "avg_creation_year = ▰▰5▰▰(▰▰6▰▰(year_created_list)/▰▰7▰▰(year_created_list))\n",
    "\n",
    "print(avg_creation_year)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.8.5"
   },
   "toc": {
    "base_numbering": 1,
    "nav_menu": {},
    "number_sections": false,
    "sideBar": true,
    "skip_h1_title": false,
    "title_cell": "Table of Contents",
    "title_sidebar": "Contents",
    "toc_cell": false,
    "toc_position": {},
    "toc_section_display": true,
    "toc_window_display": false
   },
   "varInspector": {
    "cols": {
     "lenName": 16,
     "lenType": 16,
     "lenVar": 40
    },
    "kernels_config": {
     "python": {
      "delete_cmd_postfix": "",
      "delete_cmd_prefix": "del ",
      "library": "var_list.py",
      "varRefreshCmd": "print(var_dic_list())"
     },
     "r": {
      "delete_cmd_postfix": ") ",
      "delete_cmd_prefix": "rm(",
      "library": "var_list.r",
      "varRefreshCmd": "cat(var_dic_list()) "
     }
    },
    "types_to_exclude": [
     "module",
     "function",
     "builtin_function_or_method",
     "instance",
     "_Feature"
    ],
    "window_display": false
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
