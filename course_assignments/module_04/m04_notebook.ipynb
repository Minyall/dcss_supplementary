{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<br><br><font color=\"gray\">DOING COMPUTATIONAL SOCIAL SCIENCE<br>MODULE 4 <strong>PROBLEM SETS</strong></font>\n", "\n", "# <font color=\"#49699E\" size=40>MODULE 4 </font>\n", "\n", "\n", "# What You Need to Know Before Getting Started\n", "\n", "- **Every notebook assignment has an accompanying quiz**. Your work in each notebook assignment will serve as the basis for your quiz answers.\n", "- **You can consult any resources you want when completing these exercises and problems**. Just as it is in the \"real world:\" if you can't figure out how to do something, look it up. My recommendation is that you check the relevant parts of the assigned reading or search for inspiration on [https://stackoverflow.com](https://stackoverflow.com).\n", "- **Each problem is worth 1 point**. All problems are equally weighted.\n", "- **The information you need for each problem set is provided in the blue and green cells.** General instructions / the problem set preamble are in the blue cells, and instructions for specific problems are in the green cells. **You have to execute all of the code in the problem set, but you are only responsible for entering code into the code cells that immediately follow a green cell**. You will also recognize those cells because they will be incomplete. You need to replace each blank `\u25b0\u25b0#\u25b0\u25b0` with the code that will make the cell execute properly (where # is a sequentially-increasing integer, one for each blank).\n", "- Most modules will contain at least one question that requires you to load data from disk; **it is up to you to locate the data, place it in an appropriate directory on your local machine, and replace any instances of the `PATH_TO_DATA` variable with a path to the directory containing the relevant data**.\n", "- **The comments in the problem cells contain clues indicating what the following line of code is supposed to do.** Use these comments as a guide when filling in the blanks. \n", "- **You can ask for help**. If you run into problems, you can reach out to John (john.mclevey@uwaterloo.ca) or Pierson (pbrowne@uwaterloo.ca) for help. You can ask a friend for help if you like, regardless of whether they are enrolled in the course.\n", "\n", "Finally, remember that you do not need to \"master\" this content before moving on to other course materials, as what is introduced here is reinforced throughout the rest of the course. You will have plenty of time to practice and cement your new knowledge and skills.", "\n", "<div class='alert alert-block alert-danger'>", "As you complete this assignment, you may encounter variables that can be assigned a wide variety of different names. Rather than forcing you to employ a particular convention, we leave the naming of these variables up to you. During the quiz, use the 'USER_DEFINED' option to fill in any blank that you assigned an arbitrary name to.", "</b>", "</div>"]}, {"cell_type": "markdown", "id": "delayed-antenna", "metadata": {}, "source": ["## Package Imports"]}, {"cell_type": "code", "execution_count": 1, "id": "loaded-lunch", "metadata": {}, "outputs": [], "source": ["import pickle\n", "import os\n", "from posixpath import join\n", "import random\n", "from pyprojroot import here\n", "\n", "import pandas as pd\n", "import numpy as np\n", "from scipy.stats import zscore\n", "\n", "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n", "from sklearn.decomposition import TruncatedSVD\n", "from sklearn.preprocessing import Normalizer, StandardScaler\n", "from sklearn.decomposition import PCA\n", "from sklearn.cluster import KMeans\n", "from sklearn.metrics import silhouette_score, silhouette_samples\n", "\n", "import spacy\n", "from gensim.models.phrases import Phrases, Phraser\n", "\n", "import matplotlib as mpl\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "from pprint import pprint"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 1:\n", "<div class=\"alert alert-block alert-info\">  \n", "For the first part of this assignment, we're going to work with speech data drawn from Canadian Hansards. To save you the trouble of having to work with the entire set of Hansards from 2017 through to 2020, we've already filtered them down to only include speeches by federal party leaders present in the House of Commons. Once the data has been loaded, we'll run the entire set of speeches through <code>spaCy</code>'s nlp suite, which will prepare us for the subsequent sections!\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Load the Canadian Hansard dataframe and feed the text of the speeches through <code>spaCy</code>'s nlp pipeline. Retrieve the part of speech of the first word of the speech at index 1 of the resulting list of processed speeches.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load the dataframe of leader speeches\n", "leader_df = pd.read_csv(PATH_TO_DATA/\"can_hansard_leaders.csv\")\n", "\n", "filter_terms = [\n", "    \"Justin Trudeau\", # Prime Minister, 2015-Present; Leader of the Liberal Party of Canada, 2013-Present\n", "    \"Andrew Scheer\", # Leader of the Official Opposition, 2017-2019\n", "    \"Mario Beaulieu\", # Interim Leader of the Bloc Quebequois, 2018-2019\n", "    \"Jagmeet Singh\", # Leader of the New Democratic Party, 2017-Present\n", "    \"Elizabeth May\", # Leader of the Green Party of Canada, 2006-2019\n", "    \"PPC\", # Maxime Bernier in his capacity as Leader of the People's Party of Canada, 2018-Present \n", "]\n", "\n", "# Instantiate list for dataset subsets\n", "leader_speeches = []\n", "\n", "# Iterate over filter_terms, filter dataset to term, and store resulting dataframe subset in 'leader_speeches'\n", "for t in filter_terms:\n", "    h = leader_df[leader_df[\"speakeroldname\"].str.contains(t, na=False)]\n", "    leader_speeches.append(h)\n", "\n", "# Initialize nlp pipeline, taking care to disable named entity recognition\n", "nlp = spacy.load(\u25b0\u25b00\u25b0\u25b0, disable=[\u25b0\u25b01\u25b0\u25b0])\n", "\n", "# Feed each speech through spaCy's nlp pipeline\n", "corpus = []\n", "\n", "for speech in leader_df[\u25b0\u25b02\u25b0\u25b0]:\n", "    corpus.\u25b0\u25b03\u25b0\u25b0(\u25b0\u25b04\u25b0\u25b0(speech))\n", "\n", "# Extract part of speech from the first word of the speech at index 1\n", "corpus[\u25b0\u25b05\u25b0\u25b0][\u25b0\u25b06\u25b0\u25b0].\u25b0\u25b07\u25b0\u25b0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 2:\n", "<div class=\"alert alert-block alert-info\">  \n", "Now that we have our output from <code>spaCy</code>, we have access to a wealth of information about each word in our dataset. We'll start by using this information to create a list of lists, where the outer list contains a number of inner lists, and each of the inner lists represents a single speech. We'll populate those inner lists with the lemmatized forms of all the nouns and proper nouns present in their corresponding speeches.   \n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Iterate over the docs to create a list of lists, where the outer list represents the documents and the inner list contains a list of lemmatized nouns and proper nouns. Retrieve the list of lemmatized nouns and proper nouns associated with the speech located at index 2 of the resulting list of speeches.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Populate list containing desired parts of speech, in this order:\n", "# Proper Nouns, Nouns\n", "filter_list = [\u25b0\u25b00\u25b0\u25b0, \u25b0\u25b01\u25b0\u25b0]\n", "\n", "# Initialize list for containing results\n", "lem_list = []\n", "\n", "# Iterate over speeches\n", "\u25b0\u25b02\u25b0\u25b0 speech \u25b0\u25b03\u25b0\u25b0 corpus:\n", "    # Iterate over each word in speech, add the lemmatized word if it matches one of the desired parts of speech\n", "    lem_list.\u25b0\u25b04\u25b0\u25b0([n.\u25b0\u25b05\u25b0\u25b0 for n in speech if n.\u25b0\u25b06\u25b0\u25b0 in filter_list])\n", "\n", "# Extract speech from corpus at index 2\n", "lem_list[\u25b0\u25b07\u25b0\u25b0]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 3:\n", "<div class=\"alert alert-block alert-info\">  \n", "The solution to this question is going to be very similar to that of the previous question. The major difference here is that we're going to add two new types of token to our inner lists: verbs and adjectives.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Iterate over the docs to create a list, where the outer list represents the documents and the inner list contains a list of lemmatized nouns, proper nouns, verbs, and adjectives. Retrieve the list of lemmatized nouns, proper nouns, verbs, and adjectives associated with the speech located at index 1 of the resulting list of speeches.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Populate list containing desired parts of speech, in this order:\n", "# Proper Nouns, Nouns, Verbs, Adjectives\n", "filter_list = [\u25b0\u25b00\u25b0\u25b0, \u25b0\u25b01\u25b0\u25b0, \u25b0\u25b02\u25b0\u25b0, \u25b0\u25b03\u25b0\u25b0]\n", "\n", "# Initialize list for containing results\n", "lem_list_2 = []\n", "\n", "# Iterate over speeches\n", "for speech in corpus:\n", "    # Iterate over each word in speech, add the lemmatized word if it matches one of the desired parts of speech\n", "    lem_list_2.\u25b0\u25b04\u25b0\u25b0([n.\u25b0\u25b05\u25b0\u25b0 for n in speech if n.\u25b0\u25b06\u25b0\u25b0 in filter_list])\n", "\n", "    \n", "# Extract speech from corpus at index 1\n", "lem_list_2[\u25b0\u25b07\u25b0\u25b0]\n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 4:\n", "\n", "<div class=\"alert alert-block alert-info\">  \n", "One of the more useful parsing options <code>spaCy</code> provides is robust sentence detection, which is often useful for downstream tasks and sometimes even required. One of those tasks is the bigram detection implementation in gensim. Because bigrams are usually common phrases, they often end up being some kind of topic, even if it's not the core topic of the text. In these next problems, you will create a list of the most frequently occurring bigrams for each political leader. \n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Using the corpus object you created earlier, create a flat list of all of the tokenized sentences in order to train a gensim Phrases model. While you're at it, use the leader_speeches list of dataframes to prepare a nested list of tokenized speech sentences for each political leader, which you will apply the trained bigram model on. Retrieve the first sentence by the first leader.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Initialize list for storing results\n", "sent_list = []\n", "\n", "# Iterate over speeches in corpus\n", "for speech in corpus:\n", "    # iterate over the sentences in each speech, and then the tokens from each of those sentences...\n", "    # ... add each token to 'sent_list' *as individual words, not lists*.\n", "    sent_list.\u25b0\u25b00\u25b0\u25b0([[token.\u25b0\u25b01\u25b0\u25b0 for token in sent] for sent in speech.\u25b0\u25b02\u25b0\u25b0])\n", "\n", "# Initialize list for storing results\n", "leader_sent_lists = []\n", "\n", "# Iterate over the separate leader-specific dataframes in the 'leader_speeches' variable (created in question 1)\n", "\u25b0\u25b03\u25b0\u25b0 df \u25b0\u25b04\u25b0\u25b0 leader_speeches:\n", "    \n", "    # Initialize list for storing results within the loop \n", "    leader_sentences = []\n", "    \n", "    # Run the individual dataframe (from the loop) through spacy's nlp pipe and iterate over the results\n", "    for speech in df['speechtext']:\n", "        \n", "        # iterate over the sentences in the speech, and then the tokens from each of those sentences...\n", "        # ... add each token to 'sent_list' *as individual words, not lists*.\n", "        leader_sentences.\u25b0\u25b05\u25b0\u25b0([[token.\u25b0\u25b06\u25b0\u25b0 for token in sent] for sent in \u25b0\u25b07\u25b0\u25b0(speech).\u25b0\u25b08\u25b0\u25b0])\n", "        \n", "    # Add the 'leader_sentences' list to 'leader_sent_lists' *as a list*\n", "    leader_sent_lists.\u25b0\u25b09\u25b0\u25b0(leader_sentences)\n", "\n", "# Retrieve the first sentence spoken by the first leader in the list of lists.\n", "leader_sent_lists[\u25b0\u25b010\u25b0\u25b0][\u25b0\u25b011\u25b0\u25b0]\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 5:\n", "\n", "<div class=\"alert alert-block alert-info\">  \n", "Now we can train a model and use it on the text for each leader!\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Using the sentences in the corpus, train a gensim Phrases model. Apply that model to the list that contains the list of sentences for each leader. Retrieve the bigrammed first sentence of the first speech.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Train the model using the data from Question 10\n", "model = \u25b0\u25b00\u25b0\u25b0(sent_list, min_count=1, threshold=0.75,\n", "                    scoring='npmi')  # train the model\n", "    \n", "# Create the model applicator\n", "bigrammer = Phraser(model)  \n", "\n", "# Initialize list\n", "bigrammed_list = []\n", "\n", "# Iterate over the lists in the leader_sent_lists object\n", "\u25b0\u25b01\u25b0\u25b0 sent_list \u25b0\u25b02\u25b0\u25b0 leader_sent_lists:\n", "    \n", "    # Initialize in-loop list\n", "    bigrammed_sents = []\n", "    \n", "    # Iterate over each sentence in 'sent_list'\n", "    \u25b0\u25b03\u25b0\u25b0 sent \u25b0\u25b04\u25b0\u25b0 sent_list:\n", "        \n", "        # Subscript the bigrammer with the sentence and store result \n", "        bigrammed_sent = bigrammer[sent]\n", "        \n", "        # Add the bigrammed sentence to list of bigrammed sentences\n", "        bigrammed_sents.\u25b0\u25b05\u25b0\u25b0(bigrammed_sent)\n", "    \n", "    # Add list of bigrammed sentences from leader to 'bigrammed_list'\n", "    bigrammed_list.\u25b0\u25b06\u25b0\u25b0(bigrammed_sents)\n", "    \n", "# Extract first sentence from first leader in list of lists ('bigrammed_list')\n", "bigrammed_list[\u25b0\u25b07\u25b0\u25b0][\u25b0\u25b08\u25b0\u25b0]\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 6:\n", "\n", "<div class=\"alert alert-block alert-info\">  \n", "Given that the speeches in your list of lists should - if everything went according to plan - appear in the same order as they did in the list of dataframes we used at the beginning of this part, you should be able to match up the list of bigrammed sentences to tell who's doing the talking. \n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "Print the 10 most common bigrams for each leader using a Pandas series. Hint: each leader has a list of tokenized sentences, where each sentence is a list of tokens and bigram tokens are two words joined by \"_\". Submit the name of the leader that talked about Donald Trump a lot. <br><br> Note that this question, if properly completed, should result in <b>exactly one</b> leader whose top 10 bigrams contains 'Donald_Trump'. If there are 0, 2, or more than 2 leaders who qualify, that is a strong indication that something has gone awry; try restarting the kernel and running each cell of the assignment exactly once, in order.  \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["", "\n", "# Zip together 'filter_terms' and 'bigrammed_list', and iterate over them\n", "for leader, sent_list in \u25b0\u25b00\u25b0\u25b0(filter_terms, bigrammed_list):\n", "    \n", "    # Initialize list\n", "    bigrams = []\n", "    \n", "    # Iterate over sentences in sentence list (from bigrammed_list)\n", "    for sent in sent_list:    \n", "        \n", "        # Iterate over tokens in sentence and extract bigrams...\n", "        # ... which can be identified by the presence of an underscore '_' ...\n", "        # ... and add them to the 'bigrams' list *as a list* \n", "        bigrams.\u25b0\u25b01\u25b0\u25b0([token \u25b0\u25b02\u25b0\u25b0 token \u25b0\u25b03\u25b0\u25b0 sent \u25b0\u25b04\u25b0\u25b0 '\u25b0\u25b05\u25b0\u25b0' \u25b0\u25b06\u25b0\u25b0 token])\n", "        \n", "    # Convert list of bigrams into a pandas series\n", "    bigram_series = pd.Series(bigrams)\n", "    \n", "    # Print the leader's top ten most-spoken bigrams\n", "    print(leader + '\\n')\n", "    print(bigram_series.value_counts()[:10])\n", "    print('\\n')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 7:\n", "<div class=\"alert alert-block alert-info\">  \n", "For the following exercises, you will use a dataframe containing a sample of speeches from the Canadian Hansard data. The speech text is already pre-processed, with bigrams detected and all but nouns, proper nouns, and adjectives filtered out. Using the text from the 'preprocessed' column of the dataframe, you will create a matrix of count vectors, where each row is a speech and the columns are word counts for each item in the vocabulary. You'll then create a dataframe from this matrix, adding a column with the party of the person who made the speech.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Begin by reading the CSV into pandas and extracting the pre-processed speeches as a list. Provide this list of speeches to sklearn's CountVectorizer, turning the results into a new dataframe and naming the columns for the terms they contain counts of. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# read pre-processed dataset\n", "df = pd.read_csv(PATH_TO_DATA/'processed_can_hansards.csv')\n", "# make a list of speeches\n", "speeches = df['preprocessed'].tolist()                 \n", "\n", "# initialize the counter vectorizer\n", "count_vectorizer = \u25b0\u25b00\u25b0\u25b0(max_df=.1,          \n", "                                   min_df=3,\n", "                                   strip_accents='ascii',\n", "                                   )\n", "\n", "# apply the vectorizer to the list of speeches\n", "count_matrix = count_vectorizer.\u25b0\u25b01\u25b0\u25b0(speeches)   \n", "\n", "# gather a list of the feature names (terms)\n", "vocabulary = count_vectorizer.\u25b0\u25b02\u25b0\u25b0()        \n", "\n", "# use pandas sparse matrix functionality to make a dataframe\n", "count_df = pd.DataFrame.sparse.from_spmatrix(count_matrix)    \n", "\n", "# name the columns according to what they're counting\n", "count_df.\u25b0\u25b03\u25b0\u25b0 = vocabulary                            \n", "\n", "# add the party names from the original dataframe to the new one\n", "count_df['speakerparty'] = df['speakerparty']     "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 8:\n", "<div class=\"alert alert-block alert-info\">  \n", "Combining the count vectors for each of the 3 largest political parties in Canada (Liberal, Conservative, NDP) you will produce a 3 row dataframe where the rows are the composition of each party's speeches in the data. Convert the raw counts to proportions of the party's total words, to more easily compare the party's term vectors to each other.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Use pandas' groupby to create party groups in the dataframe, with their word counts added together as the aggregation function. Transform the dataframe of counts into a dataframe of proportions (ie. term count / total words). Add the terms with the largest percentages for each party to a dictionary and print the items in the dictionary.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# group the speech vectors by party, adding the counts together\n", "party_counts = count_df.\u25b0\u25b00\u25b0\u25b0('speakerparty').\u25b0\u25b01\u25b0\u25b0('\u25b0\u25b02\u25b0\u25b0')      \n", "\n", "# transform the count dataframe to proportions by dividing the values by the total words\n", "party_percents = party_counts.div(party_counts.\u25b0\u25b03\u25b0\u25b0(axis=1), axis=0)   \n", "# Transpose the dataframe so that each party is a column and each term a row\n", "party_percents = party_percents.\u25b0\u25b04\u25b0\u25b0       \n", "\n", "top_words_per_party = {}\n", "\n", "# loop through each party in the data, adding their top (term,score) tuples to their dictionary entry \n", "for party in party_percents.\u25b0\u25b05\u25b0\u25b0:             \n", "    top = party_percents[party].\u25b0\u25b06\u25b0\u25b0(10)\n", "    top_words_per_party[party] = list(zip(top.index, top))\n", "\n", "# print the keys (party name) and associated values (top terms) in the dictionary\n", "\u25b0\u25b07\u25b0\u25b0 k, v \u25b0\u25b08\u25b0\u25b0 top_words_per_party.\u25b0\u25b09\u25b0\u25b0():\n", "    print(k.upper())\n", "    for each in \u25b0\u25b010\u25b0\u25b0:\n", "        \u25b0\u25b011\u25b0\u25b0(each)\n", "    print('\\n')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 9:\n", "<div class=\"alert alert-block alert-info\">  \n", "For this problem, you will find the words that most differentiate each party from each of the other two parties, in terms of proportion of total words. The finished code won't produce any visible output, but we'll use the three resulting dataframes (l_to_c, n_to_c, l_to_n) as the basis of a visualization we'll create in the subsequent problem.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "By subtracting the party term proportion vectors (that you created in the previous problem) from each other, gather the terms that are most associated with each side of the comparison. At this point, these vectors should be the columns of the dataframe. Because higher positive values are more associated with one party in the comparison and negative values with the other party, this only requires three comparisons to look at both ends of the party combinations. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# term vector calculations and sorting\n", "lib_to_con = \u25b0\u25b00\u25b0\u25b0['Liberal'] - \u25b0\u25b01\u25b0\u25b0['Conservative']  \n", "lib_to_con.sort_values(ascending=False, inplace=True)\n", "ndp_to_con = \u25b0\u25b02\u25b0\u25b0['NDP'] - \u25b0\u25b03\u25b0\u25b0['Conservative']\n", "ndp_to_con.sort_values(ascending=False, inplace=True)\n", "lib_to_ndp = \u25b0\u25b04\u25b0\u25b0['Liberal'] - \u25b0\u25b05\u25b0\u25b0['NDP']\n", "lib_to_ndp.sort_values(ascending=False, inplace=True)\n", "\n", "# combine the top 5 and bottom 5 values of the comparison dataframes into new ones\n", "l_to_c = pd.\u25b0\u25b06\u25b0\u25b0([lib_to_con.\u25b0\u25b07\u25b0\u25b0(), lib_to_con.\u25b0\u25b08\u25b0\u25b0()])\n", "n_to_c = pd.\u25b0\u25b09\u25b0\u25b0([ndp_to_con.\u25b0\u25b010\u25b0\u25b0(), ndp_to_con.\u25b0\u25b011\u25b0\u25b0()])\n", "l_to_n = pd.\u25b0\u25b012\u25b0\u25b0([lib_to_ndp.\u25b0\u25b013\u25b0\u25b0(), lib_to_ndp.\u25b0\u25b014\u25b0\u25b0()])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 10:\n", "<div class=\"alert alert-block alert-success\">\n", "Create a swarm plot to examine the results of the comparison between the Liberals and Conservatives. The x-axis should be the term proportions and the y-axis should be the terms themselves. Use your swarm plot to determine the word that is the most negative (Conservative) on the x-axis and submit it.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["", "\n", "fig, ax = \u25b0\u25b00\u25b0\u25b0.subplots(figsize=(6, 4))\n", "# Create a swarmplot \n", "sns.\u25b0\u25b01\u25b0\u25b0(x=l_to_c, y=l_to_c.index, color='black', size=4)\n", "# Add a vertical line at 0\n", "ax.\u25b0\u25b02\u25b0\u25b0(0) \n", "# add a grid to the plot to make it easier to interpret\n", "plt.grid()  \n", "\n", "# keep in mind which party a negative value is associated with, based on which vector was the subtracted one...\n", "ax.set(xlabel=r'($\\longleftarrow$ Conservative Party)        (Liberal Party $\\longrightarrow$)',\n", "       ylabel='',\n", "       title='Difference of Proportions')\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 11:\n", "<div class=\"alert alert-block alert-info\">  \n", "In this next batch of problems, you'll expand on the concept of the previous ones by creating TF-IDF vectors for each party and comparing them using cosine similarity. Start by creating a TF-IDF dataframe, again with the speaker party column added. Print the terms with the top TF-IDF scores after sorting by each party.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Initialize the TfidfVectorizer and implement it in a very similar way to the CountVectorizer above. At the end, print the first 10 TF-IDF scores, sorted highest to lowest, for each party. This will also print the scores for those terms for the other parties.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tfidf_vectorizer = \u25b0\u25b00\u25b0\u25b0(stop_words=\"english\",\n", "                                   lowercase=True,\n", "                                   max_features = 300,       # not best practice, we do this here in case of resource limitations\n", "                                   strip_accents='ascii')\n", "\n", "tfidf_matrix = tfidf_vectorizer.\u25b0\u25b01\u25b0\u25b0(speeches) \n", "\n", "vocabulary = tfidf_vectorizer.\u25b0\u25b02\u25b0\u25b0()\n", "\n", "tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix)\n", "tfidf_df.columns = vocabulary\n", "\n", "party_scores = tfidf_df.copy()\n", "party_scores['speakerparty'] = df['speakerparty']\n", "\n", "# group the speech vectors by party, adding the counts together\n", "party_scores = party_scores.\u25b0\u25b03\u25b0\u25b0('speakerparty').\u25b0\u25b04\u25b0\u25b0('\u25b0\u25b05\u25b0\u25b0')\n", "# Transpose the dataframe so that each party is a column and each term a row\n", "party_scores = party_scores.\u25b0\u25b06\u25b0\u25b0\n", "\n", "for party in party_scores.\u25b0\u25b07\u25b0\u25b0:\n", "    party_scores.sort_values(by = party, ascending = False, inplace = True)\n", "    print(party + '\\n')\n", "    \u25b0\u25b08\u25b0\u25b0(party_scores.head(10))\n", "    print('\\n')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 12:\n", "<div class=\"alert alert-block alert-info\">  \n", "Next you will calculate pair-wise cosine similarity to compare the vectors for each speaker in the data. You may have noticed that the TF-IDF scores from the last problem wound up being on different scales for each party, with the Liberals having the highest scores because they have the most speeches. This time, you will re-normalize the TF-IDF scores after adding them together, which also makes cosine similarity faster to calculate.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Create a new dataframe from the tfidf_matrix that you generated above. Be sure to add speakernames as usual, then filter the dataframe to keep only speeches by speakers with 50 or more speeches. Group the speeches by speaker, aggregating the TF-IDF vectors for each of their speeches, then use sklearn's Normalizer() to prepare the vectors for cosine similarity. Create a cosine similarity matrix by calculating the dot product of the normalized speaker score matrix and its transpose.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# create a new dataframe from the tfidf_matrix\n", "speaker_scores = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix)   \n", "\n", "# turn the sparse matrix into a dense one for faster aggregation runtime\n", "speaker_scores = speaker_scores.sparse.to_dense()                  \n", "\n", "# add the speaker names to the new dataframe\n", "speaker_scores[\u25b0\u25b00\u25b0\u25b0] = df[\u25b0\u25b01\u25b0\u25b0]                  \n", "\n", "# keep only speakers with 50 or more speeches to speed things up and to have vectors with a bit more term diversity\n", "speaker_scores = speaker_scores.\u25b0\u25b02\u25b0\u25b0('\u25b0\u25b03\u25b0\u25b0').\u25b0\u25b04\u25b0\u25b0(lambda x: \u25b0\u25b05\u25b0\u25b0(x) >= 50)    \n", "# group the speech vectors by speaker and aggregate their values\n", "speaker_scores = speaker_scores.\u25b0\u25b06\u25b0\u25b0('\u25b0\u25b07\u25b0\u25b0').\u25b0\u25b08\u25b0\u25b0('\u25b0\u25b09\u25b0\u25b0')        \n", "\n", "normalize = Normalizer()\n", "# convert the aggregate TF-IDF scores into unit norms\n", "speaker_scores_n = normalize.\u25b0\u25b010\u25b0\u25b0(speaker_scores)      \n", "\n", "# calculate the product of the matrix for pairwise cosine similarities\n", "speaker_matrix = speaker_scores_n @ speaker_scores_n.T         "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 13:\n", "<div class=\"alert alert-block alert-info\">  \n", "Identify the 5 most similar speakers and the 5 least similar speakers in the data.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Fill the diagonal and lower triangle of the cosine similarity matrix with np.nan values. Create a new dataframe from the matrix and make the speaker names both the index and the column names. Use df.stack() to make the dataframe 1-dimensional for a relatively simple way of finding the largest and smallest values in the whole matrix. Print the 5 highest and 5 lowest cosine comparisons. These will be the members of parliament whose speech topic composition is either most or least similar. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Fill the speaker_matrix's diagonal with NaN values\n", "np.\u25b0\u25b00\u25b0\u25b0(speaker_matrix, np.\u25b0\u25b01\u25b0\u25b0)\n", "\n", "speaker_matrix[np.tril_indices(speaker_matrix.shape[0], -1)] = np.nan\n", "speaker_df = pd.DataFrame(speaker_matrix)\n", "\n", "speaker_df.\u25b0\u25b02\u25b0\u25b0 = speaker_scores.\u25b0\u25b03\u25b0\u25b0\n", "speaker_df.\u25b0\u25b04\u25b0\u25b0 = speaker_scores.\u25b0\u25b05\u25b0\u25b0\n", "\n", "print(speaker_df.stack().\u25b0\u25b06\u25b0\u25b0(5))\n", "print('\\n')\n", "print(speaker_df.stack().\u25b0\u25b07\u25b0\u25b0(5))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 14:\n", "<div class=\"alert alert-block alert-info\">  \n", "Print the top-weighted terms for two speakers who were among the most similar to each other.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Using the normalized speaker_scores matrix, create a dataframe with speaker names as the index and feature names (terms) as the column names. Use .loc to select the row for the speaker scores you will be examining, and print the 10 most important terms along with their TF-IDF scores. Submit the word that both Anthony Rota and Bruce Stanton's share as their most important word. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["", "\n", "speaker_scores_df = pd.DataFrame(speaker_scores_n)\n", "speaker_scores_df.index = speaker_scores.index\n", "speaker_scores_df.columns = vocabulary\n", "\n", "top1 = speaker_scores_df.\u25b0\u25b00\u25b0\u25b0['Anthony Rota'].\u25b0\u25b01\u25b0\u25b0(10)\n", "top2 = speaker_scores_df.\u25b0\u25b02\u25b0\u25b0['Bruce Stanton'].\u25b0\u25b03\u25b0\u25b0(10)\n", "\n", "print(\"Anthony Rota's Top Words \\n\")\n", "\u25b0\u25b04\u25b0\u25b0(top1)\n", "print('\\n')\n", "print(\"Bruce Stanton's Top Words \\n\")\n", "\u25b0\u25b05\u25b0\u25b0(top2)"]}], "metadata": {"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": false, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}, "varInspector": {"cols": {"lenName": 16, "lenType": 16, "lenVar": 40}, "kernels_config": {"python": {"delete_cmd_postfix": "", "delete_cmd_prefix": "del ", "library": "var_list.py", "varRefreshCmd": "print(var_dic_list())"}, "r": {"delete_cmd_postfix": ") ", "delete_cmd_prefix": "rm(", "library": "var_list.r", "varRefreshCmd": "cat(var_dic_list()) "}}, "types_to_exclude": ["module", "function", "builtin_function_or_method", "instance", "_Feature"], "window_display": false}}}, "nbformat": 4, "nbformat_minor": 4}