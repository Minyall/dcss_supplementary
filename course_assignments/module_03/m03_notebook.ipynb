{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<br><br><font color=\"gray\">DOING COMPUTATIONAL SOCIAL SCIENCE<br>MODULE 3 <strong>PROBLEM SETS</strong></font>\n", "\n", "# <font color=\"#49699E\" size=40>MODULE 3 </font>\n", "\n", "\n", "# What You Need to Know Before Getting Started\n", "\n", "- **Every notebook assignment has an accompanying quiz**. Your work in each notebook assignment will serve as the basis for your quiz answers.\n", "- **You can consult any resources you want when completing these exercises and problems**. Just as it is in the \"real world:\" if you can't figure out how to do something, look it up. My recommendation is that you check the relevant parts of the assigned reading or search for inspiration on [https://stackoverflow.com](https://stackoverflow.com).\n", "- **Each problem is worth 1 point**. All problems are equally weighted.\n", "- **The information you need for each problem set is provided in the blue and green cells.** General instructions / the problem set preamble are in the blue cells, and instructions for specific problems are in the green cells. **You have to execute all of the code in the problem set, but you are only responsible for entering code into the code cells that immediately follow a green cell**. You will also recognize those cells because they will be incomplete. You need to replace each blank `\u25b0\u25b0#\u25b0\u25b0` with the code that will make the cell execute properly (where # is a sequentially-increasing integer, one for each blank).\n", "- Most modules will contain at least one question that requires you to load data from disk; **it is up to you to locate the data, place it in an appropriate directory on your local machine, and replace any instances of the `PATH_TO_DATA` variable with a path to the directory containing the relevant data**.\n", "- **The comments in the problem cells contain clues indicating what the following line of code is supposed to do.** Use these comments as a guide when filling in the blanks. \n", "- **You can ask for help**. If you run into problems, you can reach out to John (john.mclevey@uwaterloo.ca) or Pierson (pbrowne@uwaterloo.ca) for help. You can ask a friend for help if you like, regardless of whether they are enrolled in the course.\n", "\n", "Finally, remember that you do not need to \"master\" this content before moving on to other course materials, as what is introduced here is reinforced throughout the rest of the course. You will have plenty of time to practice and cement your new knowledge and skills.", "\n", "<div class='alert alert-block alert-danger'>", "As you complete this assignment, you may encounter variables that can be assigned a wide variety of different names. Rather than forcing you to employ a particular convention, we leave the naming of these variables up to you. During the quiz, use the 'USER_DEFINED' option to fill in any blank that you assigned an arbitrary name to.", "</b>", "</div>"]}, {"cell_type": "markdown", "id": "arranged-serial", "metadata": {}, "source": ["## Package Imports"]}, {"cell_type": "code", "execution_count": 32, "id": "improving-league", "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "from pprint import pprint\n", "\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.decomposition import PCA\n", "from sklearn.cluster import KMeans\n", "from sklearn.metrics import silhouette_score, silhouette_samples\n", "\n", "\n", "\n", "import matplotlib as mpl\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "%config Completer.use_jedi = False"]}, {"cell_type": "markdown", "id": "injured-boring", "metadata": {}, "source": ["## Defaults"]}, {"cell_type": "code", "execution_count": 2, "id": "confidential-horror", "metadata": {}, "outputs": [], "source": ["seed = 7"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 1:\n", "\n", "<div class=\"alert alert-block alert-info\">  \n", "In this exercise, we're going to ask you to supply the names of the Pandas methods you'll need to (1) load the .csv from disk and (2) preview a random sample of 5 rows.\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "In the code block below, fill in the blanks to insert the functions, methods, or variable names needed to load the .csv and draw a random sample of 5 rows.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load vdem_subset.csv as a dataframe\n", "df = pd.\u25b0\u25b00\u25b0\u25b0(PATH_TO_DATA/'vdem_subset.csv', low_memory=False, index_col=0)\n", "\n", "# Draw random sample of 5 rows from vdem dataframe\n", "df.\u25b0\u25b01\u25b0\u25b0(\u25b0\u25b02\u25b0\u25b0, random_state = 7)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 2:\n", "<div class=\"alert alert-block alert-info\">  \n", "You may have noticed that many of the cells in the dataframe we created have 'NaN' values. It's useful for us to know just how many values in our dataset are missing or not defined. Let's do that now:\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "In the code block below, fill in the blanks to insert the functions, methods, or variable names needed to create a Pandas series of the missing values for each column and then sort it.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Sum together all NaN values to produce series with numerical values indicating number of missing entires\n", "missing = df.\u25b0\u25b00\u25b0\u25b0().\u25b0\u25b01\u25b0\u25b0()\n", "\n", "# Sort the `missing` series\n", "missing = missing.\u25b0\u25b02\u25b0\u25b0()\n", "\n", "print(missing)\n", "print(\"Total missing values: \" + str(sum(missing)))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 3:\n", "<div class=\"alert alert-block alert-info\">\n", "The list below contains a number of variables, including mid-level indicators that go into the 5 high-level democracy indexes that were used in the assigned readings. In this problem, we'll subset our data in two ways - first conceptually by selecting only the mid-level indicators, and then empirically by selecting the indicators that heave the least missing data.\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "Use the list of column names we've provided to filter the large dataframe into a subset. Fill in the blanks to insert the functions, methods, or variable names needed.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vd_meta_vars = ['country_name', 'year', 'e_regiongeo']\n", "vd_index_vars = ['v2x_freexp_altinf', 'v2x_frassoc_thick', 'v2x_suffr', 'v2xel_frefair', 'v2x_elecoff',    # electoral democracy index\n", "              'v2xcl_rol', 'v2x_jucon', 'v2xlg_legcon',                                                 # liberal democracy index\n", "              'v2x_cspart', 'v2xdd_dd', 'v2xel_locelec', 'v2xel_regelec', 'v2x_polyarchy',              # participatory democracy index\n", "              'v2dlreason', 'v2dlcommon', 'v2dlcountr', 'v2dlconslt', 'v2dlengage',                     # deliberative democracy index\n", "              'v2xeg_eqprotec', 'v2xeg_eqaccess', 'v2xeg_eqdr']                                         # egalitarian democracy index\n", "\n", "# filter `df` so that it only includes columns from the two lists above\n", "sdf = df[\u25b0\u25b00\u25b0\u25b0 \u25b0\u25b01\u25b0\u25b0 vd_index_vars]\n", "\n", "sdf.describe()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 4:\n", "<div class=\"alert alert-block alert-info\">  \n", "Let's dig into our subsetted data. Notice that many of the variables have a range (min to max) of 0 to 1. In the next exercises we'll be making some comparisons between variables, so let's simplify those comparisons by selecting only the meta-data columns and columns with a range of 0 to 1.\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "Create a new list to filter the dataframe columns, initializing it with the names of the three meta-data columns. Append that list with the names of columns from vd_index_vars, only if the data in those columns falls within the required range. Subset the dataframe using this updated list. Fill in the blanks to continue.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sub_vd_indices = []\n", "# iteratie over the columns of vd_index_vars\n", "\u25b0\u25b00\u25b0\u25b0 column in vd_index_vars:\n", "    # filter out columns that have values greater than 1 or less than 0\n", "    \u25b0\u25b01\u25b0\u25b0 sdf[column].min() >= 0 \u25b0\u25b02\u25b0\u25b0 sdf[column].max() <= 1:\n", "        # add columns that pass the filter to a list of such columns\n", "        sub_vd_indices.append(\u25b0\u25b03\u25b0\u25b0)\n", "\n", "# create a new dataframe consisting of only those columns saved in sub_vd_indices and vd_meta_vars\n", "fsdf = sdf[vd_meta_vars \u25b0\u25b04\u25b0\u25b0 \u25b0\u25b05\u25b0\u25b0]\n", "\n", "fsdf.describe()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 5:\n", "\n", "<div class=\"alert alert-block alert-info\">\n", "In this problem set, we will continue to compute some descriptive statistics for our subsetted data and create some visualizations. We need a list that has the column names for just the variables, so we can re-use the 'subset_vd_indices' list from the previous problem. This code block will modify our dataframe so that it's easy to generate a single plot with all of the variables.\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "Use the <code>fsdf_ecdf</code> dataframe to create an empirical cumulative distribution plot of the indicator variables, using hue to differentiate them.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fsdf_ecdf = fsdf[sub_vd_indices]\n", "fsdf_ecdf = fsdf_ecdf.melt(value_vars = sub_vd_indices, var_name = 'vd_index', value_name = 'score')\n", "\n", "# create a new matplotlib figure\n", "figure = \u25b0\u25b00\u25b0\u25b0.figure(figsize=(10,6))\n", "# use seaborn to create the plot\n", "ax = sns.\u25b0\u25b01\u25b0\u25b0(fsdf_ecdf, x = \u25b0\u25b02\u25b0\u25b0, hue = \u25b0\u25b03\u25b0\u25b0, kind = \u25b0\u25b04\u25b0\u25b0)\n", "ax.set(xlabel='Score', ylabel='Proportion', title=\"ECDF for VDEM Indicator Variables\")\n", "figure.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 6:\n", "<div class=\"alert alert-block alert-info\">\n", "We can see that most of the variables follow a fairly smooth curve, while a few see dramatic proportion increases in a step-like way. This indicates that although we might consider these to be continuous variables, the measurement that produced them had some discrete (interval-like) qualities.\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "Select one variable that seems to have a relatively smooth distribution and another that has a distinctly step-like distribution. Fill their names into the underscore blanks (<code>__A__</code> and <code>__B__</code>) in the 'x' and 'y' variables in the `sns.distplot` call in the code block below. There are a number of options to choose from for each, so the distinction between the two is what matters. Produce a bivariate kernel density estimation rug plot for the two selected variables. Fill in the blanks to continue.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# create a new matplotlib figure\n", "figure = \u25b0\u25b00\u25b0\u25b0.figure()\n", "# use seaborn to create the plot\n", "ax = sns.\u25b0\u25b01\u25b0\u25b0(fsdf, x=__A__, y=__B__, kind=\u25b0\u25b02\u25b0\u25b0, rug = \u25b0\u25b03\u25b0\u25b0, rug_kws = {\"alpha\": 0.01})\n", "sns.despine()\n", "ax.set(xlabel='frassoc_thick', ylabel='polyarchy')\n", "figure.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 7:\n", "\n", "<div class=\"alert alert-block alert-info\">\n", "Now we'll create a correlation matrix (2D array) of all the variables and plot it in a heatmap to see which pairs of variables are most and least correlated. \n", "<br><br>\n", "We'll use a boolean mask to clean-up the heatmap. Remember that a boolean mask is an array of \"True\" and \"False\" values, the same size and shape as the data array, where a value of \"False\" indicates that the value in the data array should be ignored. In this case, the mask will remove all values on the top-left -> bottom-right diagonal and above that diagonal.\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "Create a heatmap of the correlation matrix. Use the heatmap to select a few correlations to print. \n", "Looking at the heatmap above, select 2 pairs of variables (4 variables total) that appear highly correlated with each other. Then, select another 2 pairs of variables (4 variables total) that appear minimally correlated with each other. Create two lists, one for the first element of each variable pair and one for the second element of each pair. These lists should be aligned. Replace the underscore blanks (<code>__A__</code> and <code>__B__</code>) with your aligned lists of variables. Jointly iterate over the two lists and print the resulting Pearson correlations between each variable pair. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fsdf_corr = fsdf[sub_vd_indices].corr()\n", "# create the upper triangular mask\n", "mask = \u25b0\u25b00\u25b0\u25b0(np.\u25b0\u25b01\u25b0\u25b0(fsdf_corr, dtype = bool))\n", "figure = plt.figure()\n", "# create the masked heatmap\n", "ax = sns.\u25b0\u25b02\u25b0\u25b0(data = fsdf_corr, \u25b0\u25b03\u25b0\u25b0 = \u25b0\u25b04\u25b0\u25b0)\n", "figure.show()\n", "\n", "var_1_list = __A__\n", "var_2_list = __B__\n", "\n", "for v1, v2 in \u25b0\u25b05\u25b0\u25b0(var_1_list, var_2_list):\n", "    result = \u25b0\u25b06\u25b0\u25b0[v1].\u25b0\u25b07\u25b0\u25b0(\u25b0\u25b08\u25b0\u25b0[v2])\n", "    print('Correlation of ' + v1 + ' and ' + v2 + ' : ' + str(\u25b0\u25b09\u25b0\u25b0))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 8:\n", "\n", "<div class=\"alert alert-block alert-info\">\n", "One useful thing that using Pandas dataframes enables us to do is group data based on one or more the columns and then work with the resulting grouped dataframe (in much the same way we would with an un-grouped dataframe). Using the VDEM data, we'll only import a subset of the data, using the 'columns_to_use' variable. At the same time, we're going to replace the numerical values in the 'e_regionpol_6c' variable with easy-to-read string representations. Finally, we'll filter the resulting dataset to include only those rows from the year 2015.<br><br>\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "In this next code block, we're going to load in a dataset, filtering our dataframe to include only those rows where the year is 2015. Fill in the blanks to continue.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["columns_to_use = [\n", "    'country_name',\n", "    'country_id',\n", "    'year',\n", "    'e_area',\n", "    'e_regionpol_6C',\n", "    'v2x_polyarchy',\n", "    'v2x_libdem',\n", "    'v2x_partipdem',\n", "    'v2x_delibdem',\n", "    'v2x_egaldem'\n", "]\n", "\n", "# Load the dataset as a dataframe\n", "df = pd.\u25b0\u25b00\u25b0\u25b0(\n", "    PATH_TO_DATA/\"vdem_subset.csv\",\n", "    usecols = \u25b0\u25b01\u25b0\u25b0,\n", "    low_memory = False\n", ")\n", "\n", "df['e_regionpol_6C'].replace({\n", "    1.0: \"East Europe and Central Asia\",\n", "    2.0: \"Latin America and Carribean\",\n", "    3.0: \"Middle East and North Africa\",\n", "    4.0: \"Sub-Saharan Africa\",\n", "    5.0: \"West Europe and North America\",\n", "    6.0: 'Asia and Pacific'\n", "}, inplace=True)\n", "\n", "\n", "# Subset the dataframe to include only those rows from 2015\n", "df_2015 = df.\u25b0\u25b02\u25b0\u25b0(\"\u25b0\u25b03\u25b0\u25b0 \u25b0\u25b04\u25b0\u25b0 2015\")\n", "\n", "df_2015"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 9:\n", "<div class=\"alert alert-block alert-info\">\n", "Now, we're going to use the Pandas Dataframe's `groupby` method to combine each nation into the region it belongs to. As you would have read in the accompanying chapter, the Pandas groupby method only preserves columns that you give it instructions for; everything else is dropped in the resulting dataframe. \n", "<br><br>\n", "In order to figure out how to aggregate each of our columns, let's think through them together. First up, we have 'country_name' and 'country_ID'. Since we're going to be grouping our data into only 6 rows (one for each of the 6 politico-geographical regions), it doesn't make sense to keep either of these columns. The same goes for 'year', since we will have already filtered our dataset to only include rows that are from 2015. We're going to be using 'e_regionpol_6C' as the basis for our groupings, so it doesn't make sense to keep it as a data column any longer. \n", "<br><br>\n", "That leaves us with 'e_area' and the 5 democracy indices. Since we're interested in knowing the total area of each region, it would make sense to <b>add</b> each country's area together. We could do something similar for the 5 democracy indices, but we'll leave them alone for now. In order to make things easier on ourselves, we're going to start by filtering out all of the columns we don't want in our final dataset, which will make aggregating what's left much easier.\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "In the following code cell, we're going to filter out most of the columns in `df_2015` so that only 'e_regionpol_6C' and 'e_area' remain, and store the resulting filtered dataframe as `df_area`. Then, we're going to run a `groupby` operation on the `e_regionpol_6C` column of and sum the `e_area` column in the `df_area` dataframe. Fill in the blanks to continue. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Filter out all columns except 'e_regionpol_6C', 'e_area'\n", "df_area = \u25b0\u25b00\u25b0\u25b0[[\u25b0\u25b01\u25b0\u25b0, 'e_area']]\n", "\n", "# group by political region and sum remaining columns\n", "df_grouped_area = df_area.\u25b0\u25b02\u25b0\u25b0(\u25b0\u25b03\u25b0\u25b0).\u25b0\u25b04\u25b0\u25b0()\n", "\n", "df_grouped_area"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 10:\n", "\n", "\n", "<div class=\"alert alert-block alert-info\">\n", "In the last question, we explored how we could use Pandas to group rows of a dataframe according to a variable's value, and to handle a subset of the remaining columns according to some kind of aggregation logic (such as adding the values or averaging over them). This time, rather than lumping countries together by region, we're going to drill deeper on how an individual nation has changed over time. For this exercise, we're going to look at how democratic norms in Costa Rica have developed in the decades since the Second World War. Since we already have the full dataframe stored in memory (as 'df'), we'll start by filtering our dataset to include only those rows pertaining to Costa Rica (across all years, not just 2015). \n", "<br><br>\n", "If you examine the resulting dataframe, you might notice that Costa Rica does not have any scores for the 5 democratic indices the earlier years for which it is present in the dataset. This should come as no surprise; even for a group as capable as the VDEM project, constructing a democratic index for the year 1839 would involve enough guesswork to render the result meaningless. As such, we're going to immediately filter our Costa Rica-only dataframe to weed out any rows that don't have scores for the 5 democratic indices. \n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Find the first year for which we have a complete set of the democratic indices for Costa Rica. Fill in the blanks to continue.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Filter the dataframe to include only rows pertaining to Costa Rica\n", "df_cr = df.\u25b0\u25b00\u25b0\u25b0(\"\u25b0\u25b01\u25b0\u25b0 \u25b0\u25b02\u25b0\u25b0 'Costa Rica'\")\n", "\n", "# Drop each row with one or more missing values\n", "df_cr_filtered = df_cr.\u25b0\u25b03\u25b0\u25b0(subset=[\n", "    'v2x_polyarchy',\n", "    'v2x_libdem',\n", "    'v2x_partipdem',\n", "    'v2x_delibdem',\n", "    'v2x_egaldem'])\n", "\n", "# Find first year for which VDEM has a complete set of indices for Costa Rica\n", "first_year = \u25b0\u25b04\u25b0\u25b0(df_cr_filtered[\u25b0\u25b05\u25b0\u25b0])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 11:\n", "<div class=\"alert alert-block alert-info\">\n", "Now our data is ready to be plotted! In this part of the exercise, we're going to plot two of Costa Rica's democratic indices against the 'year' variable to see how its democratic norms have evolved over time. We'll accomplish this by using Seaborn and taking advantage of the fact that the columns in Pandas Dataframes can be individually 'pulled out' as a Series (which operate similarly to Numpy arrays, for most intents and purposes). In the following code cell, we'll create the plot for you so you can see how it's done and what it should look like. It won't be graded, and there aren't any blanks to fill in.\n", "<br><br>Despite being as simple as can be, that doesn't look half bad! It's always a good idea to label your axes and give the plot a title so that anyone encoutering it for the first time can rapidly determine what the plot represents.\n", "<br><br>A quick note; if you want to see what the first label-less plot looks like before adding labels to the second plot, you can comment out each of the lines below the first instance of <code>figure.show()</code>.\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "Add useful labels to the x-axis and y-axis of the second plot produced by the code cell below, along with a title describing what the plot is. Fill in the blanks to continue. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cr_years = df_cr_filtered['year']\n", "cr_polyarchy = df_cr_filtered['v2x_polyarchy']\n", "\n", "figure = plt.figure(figsize=(10, 6))\n", "sns.lineplot(x = cr_years, y = cr_polyarchy)\n", "figure.show()\n", "\n", "figure = plt.figure(figsize=(10, 6))\n", "sns.lineplot(x = cr_years, y = cr_polyarchy)\n", "# Label y-axis\n", "plt.ylabel(\u25b0\u25b00\u25b0\u25b0)\n", "# Label x-axis\n", "plt.\u25b0\u25b01\u25b0\u25b0(\u25b0\u25b02\u25b0\u25b0)\n", "# Add title\n", "\u25b0\u25b03\u25b0\u25b0.\u25b0\u25b04\u25b0\u25b0(\"Polyarchy over Time, Costa Rica\")\n", "figure.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 12:\n", "\n", "<div class=\"alert alert-block alert-info\">\n", "In this exercise, we're going to work through how to combine multiple pandas dataframes. This will come in handy whenever you want to explore the relationships between variables that come from different datasets, but which can be linked according to some underlying relationship. \n", "<br><br>\n", "Earlier, we used addition to aggregate the land area of every nation in a politico-geographic region to give us a sense of how large each region was. In this exercise, we're going to turn our attention to the 5 democracy indices. Using addition (which is what we did with area) to aggregate the 5 democracy indices doesn't make as much sense, though: that might lead us to conclude that regions with more countries would be 'more democratic' than those with only a small number of nations. Instead, we'll *average* over these indicators, which will give us a sense of how democratic each region is, taken together. \n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Create a dataframe that only includes the columns we care about (the region variable and the 5 democratic indices), group the result by region, and take the average across each score. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_democracy = df_2015[['v2x_polyarchy',\n", "    'v2x_libdem',\n", "    'v2x_partipdem',\n", "    'v2x_delibdem',\n", "    'v2x_egaldem',\n", "     'e_regionpol_6C']]\n", "\n", "# Group by region and take average of other variables\n", "df_grouped_democracy = \u25b0\u25b00\u25b0\u25b0.\u25b0\u25b01\u25b0\u25b0('\u25b0\u25b02\u25b0\u25b0').\u25b0\u25b03\u25b0\u25b0()\n", "\n", "df_grouped_democracy "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 13:\n", "<div class=\"alert alert-block alert-info\">\n", "If you compare the 'df_grouped_democracy' dataframe and the 'df_grouped_area' dataframe, you might notice that the bolded columns on the left are identical. You may recall that the bold column on the left of a dataframe is the 'index', and we can take advantage of its special status to join the two dataframes together. The result will be one dataframe with the same number of rows, but with all 6 of the columns we aggregated: area and the 5 democratic indices.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "In the following code block, we're going to concatenate `df_grouped_democracy` and `df_grouped`area. Fill in the blanks to continue. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Concatenate df_grouped_democracy and df_grouped_area on rows\n", "df_full_rows = pd.\u25b0\u25b00\u25b0\u25b0([df_grouped_democracy, df_grouped_area], \u25b0\u25b01\u25b0\u25b0=1)\n", "\n", "df_full_rows"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 14:\n", "<div class=\"alert alert-block alert-info\">\n", "In the above exercise, we combined dataframes along their rows, using the row index to guide how the data was combined. We can do much the same with columns. To demonstrate how, let's return to our Costa Rica dataframe and add another country to it. Since Costa Rica and Nicaragua are geographic neighbours, it makes sense to compare them directly. \n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Time to give Nicaragua the same treatment as we did to Costa Rica! Once that's done, we're going to concatenate `df_nicaragua_filtered` and `df_nicaragua_cr`, column-wise. Fill in the blanks to continue.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create a dataframe only containing rows pertaining to Nicaragua\n", "df_nicaragua = df.\u25b0\u25b00\u25b0\u25b0(\"\u25b0\u25b01\u25b0\u25b0 \u25b0\u25b02\u25b0\u25b0 'Nicaragua'\")\n", "\n", "# Drop rows in the Nicaragua dataframe that contain NaNs in the 5 index columns \n", "df_nicaragua_filtered = df_nicaragua.\u25b0\u25b03\u25b0\u25b0(\u25b0\u25b04\u25b0\u25b0=[\n", "    'v2x_polyarchy',\n", "    'v2x_libdem',\n", "    'v2x_partipdem',\n", "    'v2x_delibdem',\n", "    'v2x_egaldem'])\n", "\n", "# Concatenate df_grouped_democracy and df_grouped_area on columns\n", "df_nicaragua_cr = pd.\u25b0\u25b05\u25b0\u25b0([df_nicaragua_filtered, \u25b0\u25b06\u25b0\u25b0], \u25b0\u25b07\u25b0\u25b0 = \u25b0\u25b08\u25b0\u25b0)\n", "\n", "df_nicaragua_cr"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 15:\n", "<div class=\"alert alert-block alert-info\">\n", "Now that the data for these two countries has been combined into a single dataframe, we can easily create plots that allow us to compare them. Again, we'll be using the Seaborn package to do our plotting for us. Even though all of our data is lumped together, Seaborn allows us to use the 'hue' variable to differentiate the data we're plotting based on some categorical variable (which, in this case, is the country variable -- it's what differentiates between Costa Rica and Nicaragua). \n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Create a line plot that contains separate lines for both Nicaragua's and Costa Rica's polyarchy score by year. We'll also include labels for the x-axis, y-axis, and plot title. Fill in the blanks to continue.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["concat_years = df_nicaragua_cr['year']\n", "concat_polyarchy = df_nicaragua_cr['v2x_polyarchy']\n", "concat_country = df_nicaragua_cr['country_name']\n", "\n", "figure = plt.figure(figsize=(10,6))\n", "ax = sns.\u25b0\u25b00\u25b0\u25b0(x=\u25b0\u25b01\u25b0\u25b0,\n", "             y=\u25b0\u25b02\u25b0\u25b0,\n", "             hue=\u25b0\u25b03\u25b0\u25b0\n", "            )\n", "ax.set(xlabel='Year', ylabel='Polyarchy', title=\"Polyarchy over Time\")\n", "\n", "figure.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 16:\n", "\n", "\n", "<div class=\"alert alert-block alert-info\">\n", "Pandas and Numpy have been built to interoperate with one another smoothly. Pandas Dataframes and multidimensional Numpy arrays can be interoperable (although with a different set of features); the same goes for Pandas Series and unidimensional Numpy arrays. Let's return to the 'df_grouped_democracy' dataframe we made earlier; it will be useful for exploring how numpy handles multidimensional arrays.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "In this exercise, we're going to turn our Pandas dataframe into a 6-by-5 Numpy array, and convert every number it contains into a whole-number percentage (which we can accomplish by multiplying by 100 and rounding to the nearest whole number). Fill in the blanks to continue.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Convert the numerical columns of our dataframe to array format \n", "arr_dem = \u25b0\u25b00\u25b0\u25b0.\u25b0\u25b01\u25b0\u25b0(df_grouped_democracy)\n", "\n", "# Multiply every value in the array by 100\n", "arr_dem_percent = arr_dem \u25b0\u25b02\u25b0\u25b0 100\n", "\n", "# Round each value in the array to the nearest whole number\n", "arr_dem_percent_r = \u25b0\u25b03\u25b0\u25b0.\u25b0\u25b04\u25b0\u25b0(arr_dem_percent)\n", "\n", "arr_dem_percent_r"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 17:\n", "<div class=\"alert alert-block alert-info\">\n", "If you compare the results from the Numpy array we produced with the values in the Pandas dataframe, you'll notice that they're more-or-less a perfect match (differing only due to rounding). We can also use Numpy to rapidly and simply perform linear algebra calculations. If, for example, we wanted to see how the polyarchy and liberal democracy variables covary with one another across the regions, we can produce a covariance matrix.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "In the code cell below, we're going to use Numpy to create a covariance matrix for polyarchy and liberal democracy. Fill in the blanks to continue. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Isolate the polyarchy column from the array\n", "arr_polyarchy = arr_dem[\u25b0\u25b00\u25b0\u25b0,\u25b0\u25b01\u25b0\u25b0]\n", "# Isolate the liberal democracy column from the array\n", "arr_libdem = arr_dem[\u25b0\u25b02\u25b0\u25b0,\u25b0\u25b03\u25b0\u25b0]\n", "\n", "# Compute the covariance of the two\n", "\u25b0\u25b04\u25b0\u25b0.\u25b0\u25b05\u25b0\u25b0(arr_polyarchy, \u25b0\u25b06\u25b0\u25b0)\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 18:\n", "<div class=\"alert alert-block alert-info\">  \n", "For the remainder of the assignment, we're going to be working with data from the European Values Survey (EVS). The data is comprised of data collected by interviewers, and was drawn from most European nations in 2017. We'll start by loading a subset of the EVS and then standardizing each of the variables. Standardized data is a <i>sine qua non</i> when working with latent variables!\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Standardize the data present in the columns of the <code>evs_df</code> dataframe. To accomplish this, use the <code>StandardScaler</code> class from the scikit-learn package. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["evs_df = pd.read_csv(PATH_TO_DATA/\"evs_subset.csv\")\n", "\n", "country_index = evs_df['country'].to_numpy()\n", "\n", "# Drop the country column, as it cannot be standardized\n", "evs_df = evs_df.\u25b0\u25b00\u25b0\u25b0(\"country\", axis=\u25b0\u25b01\u25b0\u25b0)\n", "\n", "# Standardize the data\n", "X = \u25b0\u25b02\u25b0\u25b0().\u25b0\u25b03\u25b0\u25b0(evs_df)\n", "\n", "X"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 19:\n", "<div class=\"alert alert-block alert-info\">  \n", "We'll proceed with by using our now-standardized data as the basis for a principal components analysis. Unlike in the chapter, however, we're only going to have our PCA return the top 10 components.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Perform a principal components analysis on the standardizd EVS data. Only return the top 10 components (sorted in order of explained variance ratio). Submit a numpy array containing the explained variance ratios of the 10 principal components you found. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create PCA with 10 components\n", "pca = \u25b0\u25b00\u25b0\u25b0(\u25b0\u25b01\u25b0\u25b0, random_state=42)\n", "\n", "# Fit the PCA\n", "pca.\u25b0\u25b02\u25b0\u25b0(\u25b0\u25b03\u25b0\u25b0)\n", "\n", "# Extract explained variance ratio\n", "evr = pca.\u25b0\u25b04\u25b0\u25b0\n", "\n", "print(evr)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 20:\n", "<div class=\"alert alert-block alert-info\">  \n", "Although the process of interpreting screeplots is generally subjective and open to interpretation, we're fortunate in that the screeplot from our PCA of the EVS data has a clear inflection point. It's time to flex your interpretation muscles! \n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Produce a screeplot of the 10 principal components you produced in question 2. Submit an integer corresponding to the principal component ID that corresponds with the inflection point.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["", "\n", "# Extract explained variances\n", "eigenvalues = pd.Series(pca.explained_variance_)\n", "\n", "# Create screeplot\n", "fig, ax = plt.subplots()\n", "sns.lineplot(x=eigenvalues.index, y=eigenvalues, data=eigenvalues)\n", "plt.scatter(x=eigenvalues.index, y=eigenvalues)\n", "ax.set(xlabel='Principal component ID', ylabel='Eigenvalue')\n", "sns.despine()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 21:\n", "<div class=\"alert alert-block alert-info\">  \n", "In the following code cell, we're going to ask you to perform a K-means cluster with a K of 10, AND to write a function capable of telling us a bit more about the nationalities of the individuals from each cluster. That's a lot of code for you to write, but don't be intimidated: you can treat this as two separate problems that you need to solve sequentially (consider commenting out all of the hint code regarding the function while you're working on creating the K-means analysis).\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Perform a K-means cluster analysis, where K = 10, on the EVS data. Store the labels from your K-means cluster analysis in the 'cluster' variable (provided for you). Then, write a function that zips together your list of cluster assignments and the list of countries from the EVS data (both should be the same length), and then iterates over this zipped list in order to create a dictionary where each key is a cluster number (0 through 9) and each value is a list of the 5 nationalities that most frequently appear in that cluster.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Set number of clusters\n", "num_clusters = 10\n", "\n", "# Instantiate k-means \n", "km = \u25b0\u25b00\u25b0\u25b0(n_clusters=\u25b0\u25b01\u25b0\u25b0, init='k-means++', random_state=42)\n", "\n", "# Fit instantiated k-means to data\n", "k_means_Fitted = km.fit(X)\n", "\n", "# Extract clusters\n", "\n", "clusters = k_means_Fitted.labels_.tolist() # Do not change this line\n", "\n", "# Define function for summarizing results of k-means clustering\n", "\u25b0\u25b02\u25b0\u25b0 top_countries_by_cluster(\n", "    km, \n", "    num_clusters, \n", "    country_index, \n", "    return_top = 5):\n", "\n", "    # Create zipped list of k-means labels and the country column from original dataset\n", "    cluster_countries = \u25b0\u25b03\u25b0\u25b0(\u25b0\u25b04\u25b0\u25b0(km.labels_, country_index))\n", "    \n", "    # Initialize cluster dictionary\n", "    cluster_count = {i:{} for i in range(num_clusters)}\n", "\n", "    # Iterate over cluster-country pairs:\n", "    for pair in cluster_countries:\n", "        \n", "        # Extract cluster and country\n", "        cluster_num = pair[0]\n", "        country = pair[1]\n", "\n", "        # Retrieve current count of particular nationality in cluster\n", "        current_count = cluster_count[cluster_num].get(country, 0)\n", "        \n", "        # Increment count by one \n", "        current_count \u25b0\u25b05\u25b0\u25b0 \u25b0\u25b06\u25b0\u25b0\n", "        \n", "        # Store incremented count as new value \n", "        cluster_count[cluster_num][country] = current_count\n", "    \n", "    # Sort the values of the cluster dictionary and filter down to 5 most common nationalities for each cluster\n", "    for cluster_num, country_dict in cluster_count.items():\n", "        cluster_count[cluster_num] = \u25b0\u25b07\u25b0\u25b0(country_dict, key=\u25b0\u25b08\u25b0\u25b0 x: country_dict[x], reverse=\u25b0\u25b09\u25b0\u25b0)[0:return_top]\n", "\n", "    # Return final count dictionary\n", "    return cluster_count\n", "  \n", "pprint(top_countries_by_cluster(k_means_Fitted, num_clusters, country_index))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 22:\n", "<div class=\"alert alert-block alert-info\">  \n", "In the previous question, you familiarized yourself with K-means clustering and built a function capable of summarizing some of your results. Of course, the number of clusters we used above was arbitrary, and might be nowhere close to the optimal number of clusters needed for the data. In this question, we're going to use silhouette analysis to determine a good number of centroids to cluster the data around. It's highly likely that our silhouette analysis will indicate that 2 or 3 clusters is optimal, but since we want to tease out regional variations, we'll insist on finding an optimal solution using more than 4 clusters.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Using silhouette analysis, find an optimal number of clusters for the EVS data that's greater than 4 and less than 11. Submit the K value you have decided to use (as an integer). If the silhouette analysis was conducted properly, there should be a clear best option.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["", "\n", "# Iterate over appropriate range of clusters\n", "for i in range(\u25b0\u25b00\u25b0\u25b0, \u25b0\u25b01\u25b0\u25b0):\n", "    \n", "    # Extract value of iterator for consistency \n", "    num_clusters = i\n", "\n", "    # Run k-means with iterated number of clusters (see question 4 for detailed breakdown)\n", "    km = KMeans(n_clusters=num_clusters, init='k-means++', random_state=42)\n", "    k_means_Fitted = km.fit(X)\n", "    clusters = k_means_Fitted.labels_.tolist() # Do not change this line\n", "    \n", "    # Print cluster number\n", "    print(f\"{i} clusters:\")\n", "\n", "    # Print silhouette score\n", "    print(silhouette_score(X, clusters, metric='euclidean'))\n", "\n", "    # Extract silhouette samples\n", "    samples = silhouette_samples(X, clusters)\n", "\n", "    # Plot extracted samples\n", "    ax = sns.displot(samples, kind=\"ecdf\")\n", "    ax.set(xlabel='Silhouette Score')\n", "    sns.despine()\n", "    plt.xlim(-1, 1)\n", "    plt.axvline(x=0, linewidth=2, color='darkgray')\n", "    plt.show()"]}], "metadata": {"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": false, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}, "varInspector": {"cols": {"lenName": 16, "lenType": 16, "lenVar": 40}, "kernels_config": {"python": {"delete_cmd_postfix": "", "delete_cmd_prefix": "del ", "library": "var_list.py", "varRefreshCmd": "print(var_dic_list())"}, "r": {"delete_cmd_postfix": ") ", "delete_cmd_prefix": "rm(", "library": "var_list.r", "varRefreshCmd": "cat(var_dic_list()) "}}, "types_to_exclude": ["module", "function", "builtin_function_or_method", "instance", "_Feature"], "window_display": false}}}, "nbformat": 4, "nbformat_minor": 4}