{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# <font color=\"#49699E\" size=40>Processing Natural Language Data</font>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# LEARNING OBJECTIVES\n", "# LEARNING MATERIALS\n", "# INTRODUCTION\n", "## Package Imports"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "pd.set_option(\"display.notebook_repr_html\", False)\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "\n", "from dcss.plotting import format_axes_commas, custom_seaborn\n", "from dcss.text import bigram_process, preprocess\n", "\n", "import spacy\n", "from spacy import displacy\n", "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n", "from sklearn.decomposition import TruncatedSVD\n", "\n", "custom_seaborn()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# TEXT PROCESSING\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Getting to Know SpaCy\n"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### The SpaCy NLP Pipeline\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### The SpaCy Containers\n"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["with open('../data/txt_files/bonikowski_2017.txt', 'r') as f:\n", "    abstract = f.read()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### `Doc`s\n"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["doc = nlp(abstract)\n", "print(f'There are {len(doc)} tokens in this document.')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["from spacy.tokens import DocBin\n", "\n", "doc_export = DocBin()\n", "doc_export.add(doc)\n", "doc_export.to_disk('../data/misc/bart_bonikowski_doc.spacy')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["doc_import = DocBin().from_disk('../data/misc/bart_bonikowski_doc.spacy')\n", "docs = list(doc_import.get_docs(nlp.vocab))\n", "doc = docs[0]\n", "print(f'There are {len(doc)} tokens in this document.')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### `Token`\n", "##### `Span`\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# NORMALIZING TEXT VIA LEMMATIZATION\n"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["nlp = spacy.load('en_core_web_sm', disable=['ner'], exclude = ['lemmatizer'])\n", "lemmatizer = nlp.add_pipe('lemmatizer', config = {'mode': 'rule'})\n", "lemmatizer.initialize()"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["doc = nlp(abstract)\n", "lemmatized = [(token.text, token.lemma_) for token in doc]"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["for each in lemmatized[:100]:\n", "    if each[0].lower() != each[1].lower():\n", "        print(f'{each[0]} ({each[1]})')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# PART-OF-SPEECH TAGGING\n"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["for item in doc[:20]:\n", "    print(f'{item.text} ({item.pos_})')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["nouns = [item.text for item in doc if item.pos_ == 'NOUN']\n", "print(nouns[:20])"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["adjectives = [item.text for item in doc if item.pos_ == 'ADJ']\n", "adjectives[:20]"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["parts = ['NOUN', 'ADJ']\n", "words = [item.text for item in doc if item.pos_ in parts]\n", "words[:20]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# SYNTACTIC DEPENDENCY PARSING\n"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["sentence = nlp(\"This book is a practical guide to computational social science\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Noun Chunks \n"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["for item in list(doc.noun_chunks)[:10]:\n", "    print(item.text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Extracting Words by Dependency Labels:  Subject, Verb, Object Triplets\n"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["for sent in doc.sents:\n", "    tvdo = [(token.head.text, token.text) for token in sent if token.dep_ == 'dobj']\n", "    print(tvdo)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["from dcss.svo import subject_verb_object_triples\n", "\n", "list(subject_verb_object_triples(doc))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# CONCLUSION\n", "## Key Points \n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.2"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": false, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {"height": "calc(100% - 180px)", "left": "10px", "top": "150px", "width": "333.9130554199219px"}, "toc_section_display": true, "toc_window_display": false}, "varInspector": {"cols": {"lenName": 16, "lenType": 16, "lenVar": 40}, "kernels_config": {"python": {"delete_cmd_postfix": "", "delete_cmd_prefix": "del ", "library": "var_list.py", "varRefreshCmd": "print(var_dic_list())"}, "r": {"delete_cmd_postfix": ") ", "delete_cmd_prefix": "rm(", "library": "var_list.r", "varRefreshCmd": "cat(var_dic_list()) "}}, "types_to_exclude": ["module", "function", "builtin_function_or_method", "instance", "_Feature"], "window_display": false}}, "nbformat": 4, "nbformat_minor": 4}