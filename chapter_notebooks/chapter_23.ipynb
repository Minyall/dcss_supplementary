{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#49699E\" size=40>Developing Neural Network Models with Keras and Tensorflow</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEARNING OBJECTIVES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.notebook_repr_html\", False)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dcss.plotting import custom_seaborn\n",
    "custom_seaborn()\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(42)\n",
    "seed(42)\n",
    "\n",
    "\n",
    "columns = ['speech', 'speakername', 'party', 'constituency', 'year']\n",
    "\n",
    "uk_df = pd.read_csv(\"../data/british_hansards/hansard-speeches-v301.csv\", usecols=columns).dropna(subset=['party', 'speakername', 'speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_df = uk_df[uk_df['year'].isin([2015, 2016, 2017, 2018, 2019])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering The Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_df['party'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_df = uk_df.drop(uk_df[uk_df['party'] == 'Speaker'].index)\n",
    "uk_df['party'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(uk_df['speech']), key=lambda x: len(x))[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_df.drop(uk_df[uk_df['speech'].apply(lambda x: len(x)) < 200].axes[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorizing Affiliation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = ['Conservative']\n",
    "centre = ['Liberal Democrat']\n",
    "left = ['Labour', 'Labour (Co-op)']\n",
    "national = ['Scottish National Party']\n",
    "other = list(uk_df['party'].value_counts().axes[0].drop([*right, *left, *centre, *national]))\n",
    "\n",
    "uk_df.loc[uk_df['party'].isin(right), 'affiliation'] = \"centre-right\"\n",
    "uk_df.loc[uk_df['party'].isin(centre), 'affiliation'] = \"centre\"\n",
    "uk_df.loc[uk_df['party'].isin(left), 'affiliation'] = \"centre-left\"\n",
    "uk_df.loc[uk_df['party'].isin(national), 'affiliation'] = \"national\"\n",
    "uk_df.loc[uk_df['party'].isin(other), 'affiliation'] = \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_df['affiliation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a Stratified Sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_df_strat = uk_df.groupby(\"affiliation\", group_keys=False).apply(lambda x: x.sample(3000))\n",
    "\n",
    "uk_df_strat.affiliation.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing Speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'textcat', 'parser'])\n",
    "\n",
    "lem_speeches = []\n",
    "\n",
    "for doc in tqdm(nlp.pipe(uk_df_strat['speech']), total=15000):\n",
    "    \n",
    "    lem_speeches.append([tok.lemma_ for tok in doc if not tok.is_punct])\n",
    "\n",
    "lem_speeches_joined = []\n",
    "for speech in lem_speeches:\n",
    "    lem_speeches_joined.append(\" \".join(speech))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GETTING STARTED WITH `KERAS`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing / Prep Work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the 'Affiliation' Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_df_strat[['affiliation']].sample(5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "affiliation_encoder = LabelBinarizer()\n",
    "affiliation_encoder.fit(uk_df_strat['affiliation'])\n",
    "aff_transformed = affiliation_encoder.transform(uk_df_strat['affiliation'])\n",
    "pd.DataFrame(aff_transformed).sample(5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "speech_vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english', min_df=0.01)\n",
    "speech_transformed = speech_vectorizer.fit_transform(lem_speeches_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_vectorizer.get_feature_names()[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "X_t, X_test, y_t, y_test = train_test_split(\n",
    "    speech_transformed,\n",
    "    aff_transformed,\n",
    "    test_size = 0.1,\n",
    "    shuffle = True,\n",
    "    stratify=aff_transformed\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_t,\n",
    "    y_t,\n",
    "    test_size = 0.2,\n",
    "    shuffle = True,\n",
    "    stratify=y_t\n",
    ")\n",
    "\n",
    "## You don't need to pay much attention to the following text chunk of code - it's just\n",
    "## something we have to do to make sparse numpy arrays compatable with Keras\n",
    "\n",
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.sparse.reorder(tf.SparseTensor(indices, coo.data, coo.shape))\n",
    "\n",
    "X_train = convert_sparse_matrix_to_sparse_tensor(X_train)\n",
    "X_valid = convert_sparse_matrix_to_sparse_tensor(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END-TO-END NEURAL NETWORK MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Sequential Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_model = keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_model.add(keras.layers.InputLayer(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_model.add(keras.layers.Dense(400, activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_model.add(keras.layers.Dense(400, activation=\"relu\"))\n",
    "uk_model.add(keras.layers.Dense(400, activation=\"relu\"))\n",
    "uk_model.add(keras.layers.Dense(400, activation=\"relu\"))\n",
    "uk_model.add(keras.layers.Dense(400, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_model.add(keras.layers.Dense(5, activation='softmax'))\n",
    "uk_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling a Keras ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_model.compile(\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Care, Feeding, and Training of your ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = uk_model.fit(X_train, y_train, epochs=50, validation_data = (X_valid, y_valid), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(style=['*-','o-','^-'], \n",
    "                                   linewidth=.5, markersize=3,\n",
    "                                   figsize = (8, 8))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_model_2 = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(words),\n",
    "    keras.layers.Dense(400, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(5, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_model_2.compile(\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history2 = uk_model_2.fit(X_train, y_train, epochs=50, validation_data = (X_valid, y_valid), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lims = (0, 2)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8,16))\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(ax=ax1, style=['*-','o-','^-'], \n",
    "                                   linewidth=.5, markersize=3,)\n",
    "ax1.grid(True)\n",
    "ax1.set_ylim(lims)\n",
    "ax1.title.set_text(\"5-Layer Model\")\n",
    "\n",
    "pd.DataFrame(history2.history).plot(ax=ax2, style=['*-','o-','^-'], \n",
    "                                   linewidth=.5, markersize=3,)\n",
    "ax2.grid(True)\n",
    "ax2.set_ylim(lims)\n",
    "ax2.title.set_text(\"2-Layer Model\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(\n",
    "    uk_model_2.predict(\n",
    "        convert_sparse_matrix_to_sparse_tensor(X_test)), \n",
    "    axis=1)\n",
    "\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "conf_mat = tf.math.confusion_matrix(y_true, y_pred)\n",
    "plt.figure()\n",
    "\n",
    "# grayscale for printing\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "\n",
    "sns.heatmap(\n",
    "    np.array(conf_mat).T,\n",
    "    xticklabels=affiliation_encoder.classes_,\n",
    "    yticklabels=affiliation_encoder.classes_,\n",
    "    annot=True,\n",
    "    fmt='g',\n",
    "    cmap=cmap\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Observed\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSION\n",
    "## Key Points \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
