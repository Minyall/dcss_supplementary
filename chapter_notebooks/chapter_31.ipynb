{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#49699E\" size=40>Can We Model Meaning? Contextual Representation and Neural Word Embeddings</font>\n",
    "# LEARNING OBJECTIVES\n",
    "# LEARNING MATERIALS\n",
    "# INTRODUCTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAN WE MODEL MEANING?\n",
    "## The Distributional Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHAT ARE NEURAL WORD EMBEDDINGS?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Embeddings with Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CULTURAL CARTOGRAPHY: GETTING A FEEL FOR VECTOR SPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## King - Man + Woman $\\neq$ Queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whatlies import Embedding\n",
    "from whatlies.embeddingset import EmbeddingSet\n",
    "from whatlies.language import SpacyLanguage\n",
    "lang = SpacyLanguage('en_core_web_md')\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.notebook_repr_html\", False)\n",
    "from dcss.utils import list_files, IterSents, mp_disk\n",
    "from dcss.text import bigram_process\n",
    "\n",
    "import gensim\n",
    "from multiprocessing import Process, Manager\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from dcss.plotting import custom_seaborn\n",
    "custom_seaborn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lang['queen'] - lang['king']).plot(kind='arrow', color='lightgray', show_ops=True)\n",
    "(lang['king'] + lang['woman'] - lang['man']).plot(kind='arrow', color='lightgray', show_ops=True)\n",
    "\n",
    "lang['man'].plot(kind='arrow', color='crimson')\n",
    "lang['woman'].plot(kind='arrow', color='crimson')\n",
    "\n",
    "lang['king'].plot(kind='arrow', color='black')\n",
    "lang['queen'].plot(kind='arrow', color='black')\n",
    "\n",
    "plt.axis('off');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Queen and King: \" + str(lang['queen'].distance(lang['king'])))\n",
    "print(\"Man and Woman: \" + str(lang['man'].distance(lang['woman'])))\n",
    "print(\"Man and King: \" + str(lang['man'].distance(lang['king'])))\n",
    "print(\"Woman and King: \" + str(lang['woman'].distance(lang['king'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "king_woman_no_man = lang['king'] + lang['woman'] - lang['man']\n",
    "print(\"King and combo-vector:\" + str(lang['king'].distance(king_woman_no_man)))\n",
    "print(\"Queen and combo-vector: \" + str(lang['queen'].distance(king_woman_no_man)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the combination vector because the original ('man') would be used for the plot\n",
    "king_woman_no_man.orig = king_woman_no_man.name \n",
    "\n",
    "king_queen_man_woman_plus = EmbeddingSet(lang['king'], lang['queen'], \n",
    "                                         lang['man'], lang['woman'], king_woman_no_man)\n",
    "\n",
    "king_queen_man_woman_plus.plot_interactive(x_axis=lang[\"king\"], \n",
    "                                           y_axis=lang[\"queen\"], \n",
    "                                           axis_metric = 'cosine_similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Woman and Queen: \" + str(lang['woman'].distance(lang['queen'])))\n",
    "print(\"Woman and Queen without man: \" + str((lang['woman']-lang['man']).distance(lang['queen'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Woman and Queen without man: \" + str(Embedding('halfway', lang['woman'].vector-lang['man'].vector*0.5).distance(lang['queen'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEARNING EMBEDDINGS WITH GENSIM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = list_files(\"../data/canadian_hansards/lipad/\", 'csv')\n",
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(dataset):\n",
    "    \n",
    "    dfs = [pd.read_csv(df) for df in dataset]  \n",
    "    speeches = []\n",
    "    \n",
    "    for df in dfs:\n",
    "        speeches.extend(df['speechtext'].tolist())\n",
    "    speeches = [str(s).replace('\\n|\\r', ' ') for s in speeches]     \n",
    "    _, sentences = bigram_process(speeches, n_process = 1)    \n",
    "    sentences = '\\n'.join(sentences)  \n",
    "    \n",
    "    q.put(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Manager()\n",
    "q = m.Queue()\n",
    "mp_disk(datasets, get_sentences, '../data/txt_files/can_hansard_speeches.txt', q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/txt_files/can_hansard_speeches.txt') as file:\n",
    "    data = file.read()\n",
    "    words = data.split()\n",
    "    print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = IterSents('../data/txt_files/can_hansard_speeches.txt')\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences, size = 300, window = 4, iter = 5, \n",
    "                               sg = 0, min_count = 10, negative = 5, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = sorted(list(model.wv.vocab))\n",
    "\n",
    "with open('../models/model_vocabulary.txt', 'w') as f:\n",
    "    for v in vocabulary:\n",
    "        f.write(v)\n",
    "        f.write('\\n')\n",
    "\n",
    "model.save('../models/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('../models/word2vec.model')\n",
    "model = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARING EMBEDDINGS\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twec.twec import TWEC\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "from dcss.utils import list_files, mp_disk\n",
    "\n",
    "from tok import Tokenizer\n",
    "from gensim.utils import simple_preprocess\n",
    "from multiprocessing import Process, Manager\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning Your Vector Spaces!\n",
    "## Step 1: Train the Compass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compass_path = '../data/txt_files/can_hansard_speeches.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner = TWEC(size = 300, siter = 5, diter = 5, window = 10, sg = 0, min_count = 10, ns = 5, workers = 4)\n",
    "aligner.train_compass(compass_path, overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train a Series of Aligned Embedding Models\n",
    "### Research on Cultural Change with Temporal Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = list_files(\"../data/canadian_hansards/lipad/\", 'csv')\n",
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canadian_1990s = []\n",
    "canadian_2000s = []\n",
    "canadian_2010s = []\n",
    "\n",
    "for i in range(1990,1999):\n",
    "    year_data = '../data/canadian_hansards/lipad/' + str(i) + '/'\n",
    "    datasets_1990s = list_files(year_data, 'csv')\n",
    "    canadian_1990s.extend(datasets_1990s)\n",
    "    \n",
    "for i in range(2000,2009):\n",
    "    year_data = '../data/canadian_hansards/lipad/' + str(i) + '/'\n",
    "    datasets_2000s = list_files(year_data, 'csv')\n",
    "    canadian_2000s.extend(datasets_2000s)\n",
    "    \n",
    "for i in range(2010,2019):\n",
    "    year_data = '../data/canadian_hansards/lipad/' + str(i) + '/'\n",
    "    datasets_2010s = list_files(year_data, 'csv')\n",
    "    canadian_2010s.extend(datasets_2010s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Manager()\n",
    "q = m.Queue()\n",
    "mp_disk(canadian_1990s, get_sentences, '../data/txt_files/1990s_speeches.txt', q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Manager()\n",
    "q = m.Queue()\n",
    "mp_disk(canadian_2000s, get_sentences, '../data/txt_files/2000s_speeches.txt', q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Manager()\n",
    "q = m.Queue()\n",
    "mp_disk(canadian_2010s, get_sentences, '../data/txt_files/2010s_speeches.txt', q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1990s = aligner.train_slice('../data/txt_files/1990s_speeches.txt', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2000s = aligner.train_slice('../data/txt_files/2000s_speeches.txt', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2010s = aligner.train_slice('../data/txt_files/2010s_speeches.txt', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1990s = Word2Vec.load('../models/1990s_speeches.model')\n",
    "model_2000s = Word2Vec.load('../models/2000s_speeches.model')\n",
    "model_2010s = Word2Vec.load('../models/2010s_speeches.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1990s.wv.most_similar(positive = 'climate_change', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2000s.wv.most_similar(positive = 'climate_change', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2010s.wv.most_similar(positive = 'climate_change', topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-sectional Comparisons: Political Parties on Climate Change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liberal = ['Liberal']\n",
    "conservative = ['Conservative', 'Canadian Alliance', 'Progressive Conservative', 'Reform']\n",
    "ndp = ['New Democratic Party']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_by_party(dataset, filter_terms):\n",
    "    \n",
    "    dfs_unfiltered = [pd.read_csv(df) for df in dataset]\n",
    "    dfs = []  \n",
    "    \n",
    "    for df in dfs_unfiltered:\n",
    "        temp_df = df.dropna(subset = ['speakerparty'])\n",
    "        mask = temp_df['speakerparty'].apply(lambda x: any(party for party in filter_terms if party in x))\n",
    "        temp_df2 = temp_df[mask]\n",
    "        if len(temp_df2) > 0:\n",
    "            dfs.append(temp_df2)\n",
    "        \n",
    "    speeches = []\n",
    "    \n",
    "    for df in dfs:\n",
    "        speeches.extend(df['speechtext'].tolist())\n",
    "    speeches = [str(s).replace('\\n|\\r', ' ') for s in speeches]   # make sure everything is a lowercase string, remove newlines    \n",
    "    _, sentences = u.bigram_process(speeches)    \n",
    "    sentences = '\\n'.join(sentences)  # join the batch of sentences with newlines into 1 string\n",
    "    \n",
    "    q.put(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Manager()\n",
    "q = m.Queue()\n",
    "\n",
    "mp_disk(datasets, get_sentences_by_party, '../data/txt_files/liberal_speeches.txt', q, liberal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Manager()\n",
    "q = m.Queue()\n",
    "\n",
    "mp_disk(datasets, get_sentences_by_party, '../data/txt_files/conservative_speeches.txt', q, conservative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Manager()\n",
    "q = m.Queue()\n",
    "\n",
    "mp_disk(datasets, get_sentences_by_party, '../data/txt_files/ndp_speeches.txt', q, ndp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_liberal = aligner.train_slice('../data/txt_files/liberal_speeches.txt', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conservative = aligner.train_slice('../data/txt_files/conservative_speeches.txt', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ndp = aligner.train_slice('../data/txt_files/ndp_speeches.txt', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_liberal = Word2Vec.load('../models/liberal_speeches.model')\n",
    "model_conservative = Word2Vec.load('../models/conservative_speeches.model')\n",
    "model_ndp = Word2Vec.load('../models/ndp_speeches.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_liberal.wv.most_similar(positive = 'climate_change', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conservative.wv.most_similar(positive = 'climate_change', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ndp.wv.most_similar(positive = 'climate_change', topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSION\n",
    "## Key Points \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
