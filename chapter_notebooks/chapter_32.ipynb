{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# <font color=\"#49699E\" size=40>Named Entity Recognition, Transfer Learning, and Transformer Models</font>\n", "# LEARNING OBJECTIVES\n", "# LEARNING MATERIALS\n", "# INTRODUCTION\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# NAMED ENTITY RECOGNITION (NER)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Named Entity Recognition\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## NER, Out of the Box\n", "### Imports"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["from dcss.text import *\n", "from dcss.networks import *\n", "from dcss.utils import list_files\n", "from dcss.plotting import draw_ner_blockmodel_sfdp\n", "import spacy\n", "from graph_tool.all import *\n", "import math\n", "import pandas as pd\n", "pd.set_option(\"display.notebook_repr_html\", False)\n", "from collections import Counter\n", "nlp = spacy.load('en_core_web_sm')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["with open('../data/txt_files/ca_story.txt', 'r') as f:\n", "    full_text = [line.strip() for line in f]\n", "    full_text = \" \".join(full_text)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["doc = nlp(full_text)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["ent_types = [ent.label_ for ent in doc.ents]\n", "print('Found {} named entities'.format(len(ent_types)))\n", "Counter(ent_types).most_common()"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["Counter([str(ent) for ent in doc.ents if ent.label_ == \"GPE\"])"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["Counter([str(ent) for ent in doc.ents if ent.label_ == 'PERSON']).most_common(10)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["Counter([str(ent) for ent in doc.ents if ent.label_ == 'ORG']).most_common(10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Customizing SpaCy's Pre-trained Named Entity Recognition"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["import random\n", "random.seed(7)\n", "from spacy.training import Example"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["update_list = [\n", "    ('It was a report that drew on hours of testimony from Cambridge Analytica directors, Facebook executives and dozens of expert witnesses',\n", "     {\n", "         'entities': [(53, 72, 'ORG'), (84, 92, 'ORG')]\n", "     }),\n", "    ('Cambridge Analytica rode it out, initially, but finally called in the administrators in May',\n", "     {\n", "         'entities': [(0, 19, 'ORG')]\n", "     }),\n", "    ('In April Facebook admitted it wasn\u2019t 50 million users who had had their profiles mined',\n", "     {\n", "         'entities': [(9, 17, 'ORG')]\n", "     }),\n", "    ('Facebook published a statement saying that it had banned both Cambridge Analytica and Wylie from its platform.',\n", "     {\n", "         'entities': [(0, 8, 'ORG'), (62, 81, 'ORG'), (86, 91, 'PERSON')]\n", "     })\n", "]"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["nlp = spacy.load('en_core_web_sm')  \n", "\n", "for i in range(10):\n", "    random.shuffle(update_list)    \n", "    examples = []                  \n", "    for text, label_spans in update_list:\n", "        doc = nlp.make_doc(text)          \n", "        examples.append(Example.from_dict(doc, label_spans))  \n", "    nlp.update(examples, drop = 0.6)\n", "    \n", "trained_doc = nlp(full_text)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["Counter([str(ent) for ent in trained_doc.ents if ent.label_ == \"GPE\"])"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["Counter([str(ent) for ent in trained_doc.ents if ent.label_ == 'PERSON']).most_common(10)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["Counter([str(ent) for ent in trained_doc.ents if ent.label_ == 'ORG']).most_common(10)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["# reload a fresh version of the pre-trained model\n", "nlp = spacy.load('en_core_web_sm')  "]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["examples = create_examples(full_text)\n", "\n", "for i in range(10):\n", "    random.shuffle(examples)\n", "    nlp.update(examples, drop = 0.6)\n", "    \n", "trained_doc = nlp(full_text)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["Counter([str(ent) for ent in trained_doc.ents if ent.label_ == 'PERSON']).most_common(10)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["Counter([str(ent) for ent in trained_doc.ents if ent.label_ == 'ORG']).most_common(10)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["ent_labels = [\n", "    {'label': 'ORG', 'pattern': 'Facebook'},\n", "    {'label': 'ORG', 'pattern': 'Cambridge Analytica'}\n", "]\n", "\n", "ent_ruler = nlp.add_pipe('entity_ruler', config = {'overwrite_ents': True})\n", "ent_ruler.add_patterns(ent_labels)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["ruled_doc = nlp(full_text)\n", "Counter([str(ent) for ent in ruled_doc.ents if ent.label_ == 'ORG']).most_common(10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## NER with Transfer Learning\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# TRANSFORMER MODELS"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Hugging Face + SpaCy\n"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["nlpt = spacy.load('en_core_web_trf', exclude=['tagger','lemmatizer'])\n", "doct = nlpt(full_text)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["Counter([str(ent) for ent in doct.ents if ent.label_ == 'ORG']).most_common(10)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["Counter([str(ent) for ent in doct.ents if ent.label_ == 'PERSON']).most_common(10)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["for ent in doct.ents:\n", "    if ent.text == \"Brexit\":\n", "        print(ent.label_)\n", "        print(ent.sent.text)\n", "        for ent2 in ent.sent.ents:\n", "            print(ent2.text + ': ' + ent2.label_)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["for token in doct:\n", "    if token.text == \"Brexit\":\n", "        print(token.sent.text)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["sentence = nlpt(\"The account of a whistleblower from inside the data analytics firm that had worked in different capacities \"\n", "               \"\u2013 the details are still disputed \u2013 on the two pivotal campaigns of 2016 that gave us Brexit and Trump.\")"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["for ent in sentence.ents:\n", "        print(ent.text + \": \" + ent.label_)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["sentence = nlpt(\"It was a year ago this weekend that the Observer published the first in a series of stories, known as the Cambridge Analytica Files, \"\n", "                \"that led to parliament grappling with these questions. The account of a whistleblower from inside the data analytics firm that had \"\n", "                \"worked in different capacities \u2013 the details are still disputed \u2013 on the two pivotal campaigns of 2016 that gave us Brexit and the Trump administration.\")"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["for ent in sentence.ents:\n", "        print(ent.text + \": \" + ent.label_)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Named Entities in Context\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Sentiment Analysis with Transformers\n"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["from transformers import pipeline\n", "\n", "sentiment = pipeline('sentiment-analysis')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["sentiment('\u201cWhen you look at how, for example, the NCA [National Crime Agency] has just sat on blatant evidence of Russian interference in Brexit,\u201d Wylie says.'\n", "          '\"The Brexit angle of the Cambridge Analytica Files, the explosive revelations of a second whistleblower, Shahmir Sanni, fell inexplicably flat.')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["scores = sentiment(['\u201cWhen you look at how, for example, the NCA [National Crime Agency] has '\n", "                   'just sat on blatant evidence of Russian interference in Brexit,\u201d Wylie says.',\n", "                   '\"The Brexit angle of the Cambridge Analytica Files, the explosive revelations '\n", "                   'of a second whistleblower, Shahmir Sanni, fell inexplicably flat.'])\n", "print(scores)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["sentences = [sent.text for sent in doct.sents]\n", "scores = sentiment(sentences)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["label, score = [x['label'] for x in scores], [x['score'] for x in scores]\n", "\n", "df = pd.DataFrame()\n", "df['sentence'], df['label'], df['score'] = sentences, label, score\n", "\n", "top_pos = df[df['label'] == 'POSITIVE']['score'].idxmax()\n", "bot_pos = df[df['label'] == 'POSITIVE']['score'].idxmin()\n", "top_neg = df[df['label'] == 'NEGATIVE']['score'].idxmax()\n", "bot_neg = df[df['label'] == 'NEGATIVE']['score'].idxmin()\n", "\n", "for pos in [top_pos, bot_pos, top_neg, bot_neg]:    \n", "    print('Value: ' + str(df['score'].iloc[pos]) + '\\nSentence: ' + df['sentence'].iloc[pos], '\\n')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["sent_df = entity_sentiment(doct, sentiment, ['Cambridge Analytica'])\n", "\n", "sent_df['sent_signed'] = sent_df['sentiment_score']\n", "sent_df.loc[sent_df['sentiment'] == 'NEGATIVE', 'sent_signed'] *= -1"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["sent_df['sent_signed'].mean()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Translating Transformer Insight into Human Insight"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["datasets = list_files(\"../data/canadian_hansards/lipad/\", 'csv')\n", "dfs = [pd.read_csv(df, low_memory=False) for df in datasets]\n", "df = pd.concat(dfs)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["leaders = ['Stephen Harper', 'Jack Layton', 'Jean Chr\u00e9tien']\n", "df_filt = df[df['speakername'].isin(leaders)]\n", "df_filt.speakername.value_counts()"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["nlp = spacy.load('en_core_web_trf', exclude=['tagger', 'parser', 'lemmatizer'])\n", "nlp.add_pipe('sentencizer')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["sentiment_df = process_speeches_sentiment(df_filt, nlp, sentiment)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["sentiment_df.to_pickle('../data/pickles/can_hansard_sentiment.pkl')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["# run this cell to load the data with everything above analysed already\n", "sentiment_df = pd.read_pickle('../data/pickles/can_hansard_sentiment.pkl')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["sentiment_df['sent_signed'] = sentiment_df['sentiment_score']\n", "sentiment_df.loc[sentiment_df['sentiment'] == 'NEGATIVE', 'sent_signed'] *= -1"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["sentiment_df.value_counts(subset=['speaker', 'sentiment'], sort=False)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["sentiment_df.groupby('speaker')['sent_signed'].mean()"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["chretien_df = create_speaker_edge_df(sentiment_df, 'Jean Chr\u00e9tien')\n", "layton_df = create_speaker_edge_df(sentiment_df, 'Jack Layton')\n", "harper_df = create_speaker_edge_df(sentiment_df, 'Stephen Harper')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["chretien_df.groupby(['source','target'])['weight'].mean().reset_index().sort_values(by='weight', ascending = False)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["layton_df.groupby(['source','target'])['weight'].mean().reset_index().sort_values(by='weight', ascending = False)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["harper_df.groupby(['source','target'])['weight'].mean().reset_index().sort_values(by='weight', ascending = False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Co-Occurring Named Entities\n"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["chretien_df = create_speaker_edge_df(sentiment_df, 'Jean Chr\u00e9tien')\n", "layton_df = create_speaker_edge_df(sentiment_df, 'Jack Layton')\n", "harper_df = create_speaker_edge_df(sentiment_df, 'Stephen Harper')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["chretien_small_df = shrink_sent_df(chretien_df)\n", "layton_small_df = shrink_sent_df(layton_df)\n", "harper_small_df = shrink_sent_df(harper_df)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["chretien_small_G, chretien_small_blocks = blockmodel_from_edge_df(chretien_small_df, n_edges = 200)\n", "layton_small_G, layton_small_blocks = blockmodel_from_edge_df(layton_small_df, n_edges = 200)\n", "harper_small_G, harper_small_blocks = blockmodel_from_edge_df(harper_small_df, n_edges = 200)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["draw_ner_blockmodel_sfdp(chretien_small_G, chretien_small_blocks, filename = '../figures/chretien_blockmodel_top200_unweighted_sfdp.pdf')\n", "draw_ner_blockmodel_sfdp(layton_small_G, layton_small_blocks, filename = '../figures/layton_blockmodel_top200_unweighted_sfdp.pdf')\n", "draw_ner_blockmodel_sfdp(harper_small_G, harper_small_blocks, filename = '../figures/harper_blockmodel_top200_unweighted_sfdp.pdf')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["chretien_results = get_sentiment_blocks_df(chretien_small_G, chretien_small_blocks)\n", "layton_results = get_sentiment_blocks_df(layton_small_G, layton_small_blocks)\n", "harper_results = get_sentiment_blocks_df(harper_small_G, harper_small_blocks)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["chretien_results.head()"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["chretien_block_sentiment_df = calculate_avg_block_sentiment(chretien_results, chretien_df)\n", "chretien_block_sentiment_df.to_pickle('../data/pickles/chretien_blockmodel_sent_analysis.pkl')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["# run to load the pickled dataframe\n", "chretien_block_sentiment_df = pd.read_pickle('../data/pickles/chretien_blockmodel_sent_analysis.pkl')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["chretien_block_sentiment_df.head(30)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["layton_block_sentiment_df = calculate_avg_block_sentiment(layton_results, layton_df)\n", "layton_block_sentiment_df.to_pickle('../data/pickles/layton_blockmodel_sent_analysis.pkl')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["# run to load the pickled dataframe\n", "layton_block_sentiment_df.to_pickle('../data/pickles/layton_blockmodel_sent_analysis.pkl')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["layton_block_sentiment_df.head(30)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["harper_block_sentiment_df = calculate_avg_block_sentiment(harper_results, harper_df)\n", "harper_block_sentiment_df.to_pickle('../data/pickles/harper_blockmodel_sent_analysis.pkl')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["# run to load the pickled dataframe\n", "harper_block_sentiment_df.to_pickle('../data/pickles/harper_blockmodel_sent_analysis.pkl')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["harper_block_sentiment_df.head(30)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# CONCLUSION\n", "## Key Points \n"]}], "metadata": {"kernelspec": {"display_name": "Python [conda env:dcss]", "language": "python", "name": "conda-env-dcss-py"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": false, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}, "varInspector": {"cols": {"lenName": 16, "lenType": 16, "lenVar": 40}, "kernels_config": {"python": {"delete_cmd_postfix": "", "delete_cmd_prefix": "del ", "library": "var_list.py", "varRefreshCmd": "print(var_dic_list())"}, "r": {"delete_cmd_postfix": ") ", "delete_cmd_prefix": "rm(", "library": "var_list.r", "varRefreshCmd": "cat(var_dic_list()) "}}, "types_to_exclude": ["module", "function", "builtin_function_or_method", "instance", "_Feature"], "window_display": false}}, "nbformat": 4, "nbformat_minor": 4}