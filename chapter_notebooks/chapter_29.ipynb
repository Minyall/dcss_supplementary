{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# <font color=\"#49699E\" size=40>Variational Bayes and the Craft of Generative Topic Modelling</font>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# LEARNING OBJECTIVES\n", "# LEARNING MATERIALS\n", "# INTRODUCTION\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# GENERATIVE TOPIC MODELS\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Latent Dirichlet Allocation (LDA)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## LDA as a Graphical Model\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## The Dirichlet in Latent Dirichlet Allocation\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Understanding the $\\alpha$ hyperparameter\n", "### Understanding the $\\eta$ hyperparameter\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Variational Inference\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Selecting the Number of Topics\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# TOPIC MODELLING WITH GENSIM"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import seaborn as sns\n", "import matplotlib as mpl\n", "import matplotlib.pyplot as plt\n", "pd.set_option(\"display.notebook_repr_html\", False)\n", "from dcss.plotting import custom_seaborn\n", "custom_seaborn()\n", "\n", "from dcss.text import preprocess, bow_to_df\n", " \n", "from gensim import corpora\n", "from pprint import pprint\n", "from gensim.models import LdaModel\n", "from gensim.models.ldamulticore import LdaMulticore\n", "from gensim.models.coherencemodel import CoherenceModel\n", "import pickle"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('../data/canadian_hansards/lipad/canadian_hansards.csv', low_memory=False)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["df.info()"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["texts = df['speechtext'].tolist()\n", "processed_text = preprocess(texts, bigrams=False, detokenize=False, n_process = 32)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["len(processed_text)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["with open('../data/pickles/preprocessed_speeches_canadian_hansards_no_bigrams.pkl', 'wb') as handle:\n", "    pickle.dump(processed_text, handle, protocol=pickle.HIGHEST_PROTOCOL)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["processed_text = pickle.load( open( '../data/pickles/preprocessed_speeches_canadian_hansards_no_bigrams.pkl', 'rb'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Creating a Bag-of-Words with Gensim"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["vocab = corpora.Dictionary(processed_text) # id2word\n", "vocab.save('../models/lda_vocab.dict')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["vocab = corpora.Dictionary.load('../models/lda_vocab.dict')\n", "vocab.filter_extremes(no_below=20, no_above=0.95)\n", "corpus = [vocab.doc2bow(text) for text in processed_text]"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["len(vocab)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Running the Topic Model"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["import random\n", "random.seed(100)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["sample_corpus, sample_text = zip(*random.sample(list(zip(corpus,processed_text)),100000))"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["ldamod_s = LdaModel(corpus=sample_corpus,\n", "                      id2word=vocab,\n", "                      num_topics=100,\n", "                      random_state=100,\n", "                      eval_every=1,\n", "                      chunksize=2000,\n", "                      alpha='auto',\n", "                      eta='auto',\n", "                      passes=2,\n", "                      update_every=1,\n", "                      iterations=400\n", "                  )"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["with open('../data/pickles/lda_model_sample.pkl', 'wb') as handle:\n", "    pickle.dump(ldamod_s, handle, protocol=pickle.HIGHEST_PROTOCOL)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["ldamod_s = pickle.load(open( '../data/pickles/lda_model_sample.pkl', 'rb'))"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["ldamod_s.get_term_topics('freedom')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["ldamod_s.show_topic(53)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["ldamod_s.get_term_topics('criminal')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["ldamod_s.show_topic(20)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["ldamod_s.get_term_topics('marriage')"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["ldamod_s.show_topic(28, topn=30)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Evaluating the Quality of Topic Models by Measuring Semantic Coherence\n"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["coherence_model_s = CoherenceModel(model=ldamod_s, \n", "                                     texts=sample_text, \n", "                                     dictionary=vocab, \n", "                                     coherence='c_v')\n", "\n", "coherence_lda_s = coherence_model_s.get_coherence()\n", "print('Coherence Score: ', coherence_lda_s)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["with open('../data/pickles/coherence_model_sample.pkl', 'wb') as handle:\n", "    pickle.dump(coherence_model_s, handle, protocol=pickle.HIGHEST_PROTOCOL)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["coherence_model_s = pickle.load( open( '../data/pickles/coherence_model_sample.pkl', 'rb'))"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["topic_coherence_s = coherence_model_s.get_coherence_per_topic(with_std = True)\n", "topic_coherence_df = pd.DataFrame(topic_coherence_s, columns = ['coherence','std'])\n", "topic_coherence_df = topic_coherence_df.sort_values(['coherence', 'std'], ascending=[False,True])"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["topic_coherence_df.head(10).mean()"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["topic_coherence_df.tail(10).mean()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Going Further with Better Priors"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["alpha_asym = np.fromiter(\n", "                    (1.0 / (i + np.sqrt(100)) for i in range(100)),\n", "                    dtype=np.float16, count=100,\n", "                    )\n", "eta_sym = 1/100"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["alpha_t = ldamod_s.alpha\n", "eta_t = ldamod_s.eta"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["print(\"Trained alpha variance: \" + str(np.round(np.var(alpha_t), 4)))\n", "print(\"Asymmetric alpha variance: \" + str(np.round(np.var(alpha_asym), 4)))\n", "print(\"Trained alpha avg: \" + str(np.round(alpha_t.sum()/len(alpha_t), 4)))\n", "print(\"Asymmetric alpha avg: \" + str(np.round(alpha_asym.sum()/len(alpha_asym), 4)))\n", "\n", "print(\"Trained eta variance: \" + str(np.round(np.var(eta_t), 4)))\n", "print(\"Symmetric eta variance: \" + str(np.round(np.var(eta_sym), 4)))\n", "print(\"Trained eta avg: \" + str(np.round(eta_t.sum()/len(eta_t),4)))\n", "print(\"Symmetric eta avg: \" + str(np.round(eta_sym, 4)))"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["ldamod_f = LdaMulticore(corpus=corpus,\n", "                      id2word=vocab,\n", "                      num_topics=100,\n", "                      random_state=100,\n", "                      chunksize=2000,\n", "                      alpha=alpha_t,\n", "                      eta=eta_t,\n", "                      passes=1,\n", "                      iterations=10,\n", "                      workers=15,\n", "                      per_word_topics=True)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["with open('../data/pickles/lda_model_full.pkl', 'wb') as handle:\n", "    pickle.dump(ldamod_f, handle, protocol=pickle.HIGHEST_PROTOCOL)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["ldamod_f = pickle.load( open( '../data/pickles/lda_model_full.pkl', 'rb'))"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["coherence_model_full = CoherenceModel(model=ldamod_f,\n", "                                     texts=processed_text,\n", "                                     dictionary=vocab,\n", "                                     coherence='c_v')\n", "coherence_full = coherence_model_full.get_coherence()"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["with open('../data/pickles/coherence_model_full.pkl', 'wb') as handle:\n", "    pickle.dump(coherence_model_full, handle, protocol=pickle.HIGHEST_PROTOCOL)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["coherence_model_full = pickle.load( open( '../data/pickles/coherence_model_full.pkl', 'rb'))"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["coherence_full"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["topic_coherence_f = coherence_model_full.get_coherence_per_topic(with_std = True)\n", "topic_coherence_f_df = pd.DataFrame(topic_coherence_f, columns = ['coherence','std'])\n", "topic_coherence_f_df = topic_coherence_f_df.sort_values(['coherence', 'std'], ascending=[False,True])"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["print(\"Full model average coherence top 30 topics: \" + str(topic_coherence_f_df['coherence'].head(30).mean()))\n", "print(\"Sample model average coherence top 30 topics: \" + str(topic_coherence_df['coherence'].head(30).mean()))\n", "print(\"Full model average coherence bottom 30 topics: \" + str(topic_coherence_f_df['coherence'].tail(30).mean()))\n", "print(\"Sample model average coherence bottom 30 topics: \" + str(topic_coherence_df['coherence'].tail(30).mean()))"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["topic_coherence_f_df.head(10)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["topic_coherence_df.head(10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Visualizing Topic Model Output with PyLDAVis\n"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["import pyLDAvis.gensim_models as gensimvis\n", "from pyLDAvis import save_html"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["vis = gensimvis.prepare(ldamod_f, corpus, vocab)"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["save_html(vis, '../data/misc/ldavis_full_model.html')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# CONCLUSION\n", "## Key Points \n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.2"}}, "nbformat": 4, "nbformat_minor": 4}