{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<br><br><font color=\"gray\">DOING COMPUTATIONAL SOCIAL SCIENCE<br>MODULE 10 <strong>PROBLEM SETS</strong></font>\n", "\n", "# <font color=\"#49699E\" size=40>MODULE 10 </font>\n", "\n", "\n", "# What You Need to Know Before Getting Started\n", "\n", "- **Every notebook assignment has an accompanying quiz**. Your work in each notebook assignment will serve as the basis for your quiz answers.\n", "- **You can consult any resources you want when completing these exercises and problems**. Just as it is in the \"real world:\" if you can't figure out how to do something, look it up. My recommendation is that you check the relevant parts of the assigned reading or search for inspiration on [https://stackoverflow.com](https://stackoverflow.com).\n", "- **Each problem is worth 1 point**. All problems are equally weighted.\n", "- **The information you need for each problem set is provided in the blue and green cells.** General instructions / the problem set preamble are in the blue cells, and instructions for specific problems are in the green cells. **You have to execute all of the code in the problem set, but you are only responsible for entering code into the code cells that immediately follow a green cell**. You will also recognize those cells because they will be incomplete. You need to replace each blank `\u25b0\u25b0#\u25b0\u25b0` with the code that will make the cell execute properly (where # is a sequentially-increasing integer, one for each blank).\n", "- Most modules will contain at least one question that requires you to load data from disk; **it is up to you to locate the data, place it in an appropriate directory on your local machine, and replace any instances of the `PATH_TO_DATA` variable with a path to the directory containing the relevant data**.\n", "- **The comments in the problem cells contain clues indicating what the following line of code is supposed to do.** Use these comments as a guide when filling in the blanks. \n", "- **You can ask for help**. If you run into problems, you can reach out to John (john.mclevey@uwaterloo.ca) or Pierson (pbrowne@uwaterloo.ca) for help. You can ask a friend for help if you like, regardless of whether they are enrolled in the course.\n", "\n", "Finally, remember that you do not need to \"master\" this content before moving on to other course materials, as what is introduced here is reinforced throughout the rest of the course. You will have plenty of time to practice and cement your new knowledge and skills.", "\n", "<div class='alert alert-block alert-danger'>", "As you complete this assignment, you may encounter variables that can be assigned a wide variety of different names. Rather than forcing you to employ a particular convention, we leave the naming of these variables up to you. During the quiz, submit an answer of 'USER_DEFINED' (without the quotation marks) to fill in any blank that you assigned an arbitrary name to. In most circumstances, this will occur due to the presence of a local iterator in a for-loop.", "</b>", "</div>"]}, {"cell_type": "markdown", "id": "certified-bronze", "metadata": {}, "source": ["## Package Imports"]}, {"cell_type": "code", "execution_count": 1, "id": "devoted-divorce", "metadata": {}, "outputs": [], "source": ["import pandas as pd \n", "\n", "import numpy as np\n", "from numpy.random import seed as np_seed\n", "\n", "\n", "import graphviz\n", "from graphviz import Source\n", "\n", "from pyprojroot import here\n", "\n", "import matplotlib.pyplot as plt\n", "import matplotlib as mpl\n", "import seaborn as sns\n", "\n", "from sklearn import preprocessing\n", "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit\n", "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n", "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n", "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier\n", "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "\n", "import tensorflow as tf\n", "from tensorflow import keras\n", "from tensorflow.random import set_seed\n", "\n", "import spacy \n", "\n", "from time import time\n", "\n", "set_seed(42)\n", "np_seed(42)\n", "\n"]}, {"cell_type": "markdown", "id": "selected-dressing", "metadata": {}, "source": ["## Defaults"]}, {"cell_type": "code", "execution_count": 2, "id": "acoustic-modification", "metadata": {}, "outputs": [], "source": ["x_columns = [\n", "\n", "    # Religion and Morale\n", "    'v54', # Religious services? - 1=More than Once Per Week, 7=Never\n", "    'v149', # Do you justify: claiming state benefits? - 1=Never, 10=Always\n", "    'v150', # Do you justify: cheating on tax? - 1=Never, 10=Always \n", "    'v151', # Do you justify: taking soft drugs? - 1=Never, 10=Always \n", "    'v152', # Do you justify: taking a bribe? - 1=Never, 10=Always \n", "    'v153', # Do you justify: homosexuality? - 1=Never, 10=Always \n", "    'v154', # Do you justify: abortion? - 1=Never, 10=Always \n", "    'v155', # Do you justify: divorce? - 1=Never, 10=Always \n", "    'v156', # Do you justify: euthanasia? - 1=Never, 10=Always \n", "    'v157', # Do you justify: suicide? - 1=Never, 10=Always \n", "    'v158', # Do you justify: having casual sex? - 1=Never, 10=Always \n", "    'v159', # Do you justify: public transit fare evasion? - 1=Never, 10=Always \n", "    'v160', # Do you justify: prostitution? - 1=Never, 10=Always \n", "    'v161', # Do you justify: artificial insemination? - 1=Never, 10=Always \n", "    'v162', # Do you justify: political violence? - 1=Never, 10=Always \n", "    'v163', # Do you justify: death penalty? - 1=Never, 10=Always \n", "\n", "    # Politics and Society\n", "    'v97', # Interested in Politics? - 1=Interested, 4=Not Interested\n", "    'v121', # How much confidence in Parliament? - 1=High, 4=Low\n", "    'v126', # How much confidence in Health Care System? - 1=High, 4=Low\n", "    'v142', # Importance of Democracy - 1=Unimportant, 10=Important\n", "    'v143', # Democracy in own country - 1=Undemocratic, 10=Democratic\n", "    'v145', # Political System: Strong Leader - 1=Good, 4=Bad\n", "#     'v208', # How often follow politics on TV? - 1=Daily, 5=Never\n", "#     'v211', # How often follow politics on Social Media? - 1=Daily, 5=Never\n", "\n", "    # National Identity\n", "    'v170', # How proud are you of being a citizen? - 1=Proud, 4=Not Proud\n", "    'v184', # Immigrants: impact on development of country - 1=Bad, 5=Good\n", "    'v185', # Immigrants: take away jobs from Nation - 1=Take, 10=Do Not Take\n", "    'v198', # European Union Enlargement - 1=Should Go Further, 10=Too Far Already\n", "]\n", "\n", "y_columns = [\n", "    # Overview\n", "    'country',\n", "    \n", "    # Socio-demographics\n", "    'v226', # Year of Birth by respondent \n", "    'v261_ppp', # Household Monthly Net Income, PPP-Corrected\n", "\n", "]\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 1:\n", "<div class=\"alert alert-block alert-info\">  \n", "In this assignment, we're going to continue our exploration of the European Values Survey dataset. By wielding the considerable power of Artificial Neural Networks, we'll aim to create a model capable of predicting an individual survey respondent's country of residence. As with all machine/deep learning projects, our first task will involve loading and preparing the data.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Load the EVS dataset and use it to create a feature matrix (using all columns from x_columns) and (with the assistance of Scikit Learn's LabelBinarizer) a target array (representing each respondent's country of residence).  \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load EVS Dataset \n", "df = pd.read_csv(PATH_TO_DATA/\"evs_module_08.csv\")\n", "\n", "# Create Feature Matrix (using all columns from x_columns)\n", "X = df[x_columns] \n", "\n", "# Initialize LabelBinarizer\n", "country_encoder = \u25b0\u25b01\u25b0\u25b0()\n", "\n", "# Fit the LabelBinarizer instance to the data's 'country' column and store transformed array as target \n", "y = country_encoder.\u25b0\u25b02\u25b0\u25b0(np.array(\u25b0\u25b03\u25b0\u25b0))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 2:\n", "\n", "<div class=\"alert alert-block alert-info\">  \n", "As part of your work in the previous module, you were introduced to the concept of the train-validate-test split. Up until now, we had made extensive use of Scikit Learn's preprocessing and cross-validation suites in order to easily get the most out of our data. Since we're using TensorFlow for our Artificial Neural Networks, we're going to have to change course a little: we can still use the <code>train_test_split</code> function, but we must now use it twice: the first iteration will produce our test set and a 'temporary' dataset; the second iteration will split the 'temporary' data into training and validation sets. Throughout this process, we must take pains to ensure that each of the data splits are shuffled and stratified. \n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Create shuffled, stratified splits for testing (10% of original dataset), validation (10% of data remaining from test split), and training (90% of data remaining from test split) sets. Submit the number of observations in the <code>X_valid</code> set, as an integer.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["", "\n", "# Split into temporary and test sets\n", "X_t, X_test, y_t, y_test = \u25b0\u25b01\u25b0\u25b0(\n", "    \u25b0\u25b02\u25b0\u25b0,\n", "    \u25b0\u25b03\u25b0\u25b0,\n", "    test_size = \u25b0\u25b04\u25b0\u25b0,\n", "    shuffle = \u25b0\u25b05\u25b0\u25b0,\n", "    stratify = y,\n", "    random_state = 42\n", ")\n", "\n", "# Split into training and validation sets\n", "X_train, X_valid, y_train, y_valid = train_test_split(\n", "    \u25b0\u25b06\u25b0\u25b0,\n", "    \u25b0\u25b07\u25b0\u25b0,\n", "    test_size = \u25b0\u25b08\u25b0\u25b0,\n", "    shuffle = \u25b0\u25b09\u25b0\u25b0,\n", "    stratify = \u25b0\u25b010\u25b0\u25b0,\n", "    random_state = 42,\n", ")\n", "\n", "len(X_valid)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 3:\n", "\n", "<div class=\"alert alert-block alert-info\">  \n", "As you work with Keras and Tensorflow, you'll rapidly discover that both packages are very picky about the 'shape' of the data you're using. What's more, you can't always rely on them to correctly infer your data's shape. As such, it's usually a good idea to store the two most important shapes -- number of variables in the feature matrix and number of unique categories in the target -- as explicit, named variables; doing so will save you the trouble of trying to retrieve them later (or as part of your model specification, which can get messy). We'll start with the number of variables in the feature matrix.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Store the number of variables in the feature matrix, as an integer, in the <code>num_vars</code> variable. Submit the resulting number as an integer.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["", "# The code we've provided here is just a suggestion; feel free to use any approach you like\n", "num_vars = np.\u25b0\u25b01\u25b0\u25b0(\u25b0\u25b02\u25b0\u25b0).\u25b0\u25b03\u25b0\u25b0[1]\n", "\n", "print(num_vars)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 4:\n", "\n", "<div class=\"alert alert-block alert-info\">  \n", "Now, for the number of categories (a.k.a. labels) in the target.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Store the number of categories in the target, as an integer, in the <code>num_vars</code> variable. Submit the resulting number as an integer.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["", "# The code we've provided here is just a suggestion; feel free to use any approach you like\n", "num_labels = \u25b0\u25b01\u25b0\u25b0.\u25b0\u25b02\u25b0\u25b0[1]\n", "\n", "print(num_labels)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 5:\n", "\n", "<div class=\"alert alert-block alert-info\">  \n", "Everything is now ready for us to begin building an Artifical Neural Network! Aside from specifying that the ANN must be built using Keras's <code>Sequential</code> API, we're going to give you the freedom to tackle the creation of your ANN in whichever manner you like. Feel free to use the 'add' method to build each layer one at a time, or pass all of the layers to your model at instantiation as a list, or any other approach you may be familiar with. Kindly ensure that your model matches the specifications below <b>exactly</b>!\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Using Keras's <code>Sequential</code> API, create a new ANN. Your ANN should have the following layers, in this order:\n", "<ol>\n", "<li> Input layer with one argument: number of variables in the feature matrix\n", "<li> Dense layer with 400 neurons and the \"relu\" activation function\n", "<li> Dense layer with 10 neurons and the \"relu\" activation function\n", "<li> Dense layer with neurons equal to the number of labels in the target and the \"softmax\" activation function\n", "</ol>\n", "Submit the number of hidden layers in your model.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["", "# Create your ANN!\n", "nn_model = keras.models.Sequential()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 6:\n", "<div class=\"alert alert-block alert-info\">  \n", "Even though we've specified all of the layers in our model, it isn't yet ready to go. We must first 'compile' the model, during which time we'll specify a number of high-level arguments. Just as in the textbook, we'll go with a fairly standard set of arguments: we'll use Stochastic Gradient Descent as our optimizer, and our only metric will be Accuracy (an imperfect but indispensably simple measure). It'll be up to you to figure out what loss function we should use: you might have to go digging in the textbook to find it!\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Compile the model according to the specifications outlined in the blue text above. Submit the name of the loss function <b>exactly</b> as it appears in your code (you should only need to include a single underscore -- no other punctuation, numbers, or special characters). \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["", "nn_model.\u25b0\u25b01\u25b0\u25b0(\n", "    loss=keras.losses.\u25b0\u25b02\u25b0\u25b0,\n", "    optimizer=\u25b0\u25b03\u25b0\u25b0,\n", "    metrics=[\u25b0\u25b04\u25b0\u25b0]\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 7:\n", "<div class=\"alert alert-block alert-info\">  \n", "Everything is prepared. All that remains is to train the model! \n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Train your neural network for 100 epochs. Be sure to include the validation data variables. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np_seed(42)\n", "tf.random.set_seed(42)\n", "\n", "history = nn_model.\u25b0\u25b01\u25b0\u25b0(\u25b0\u25b02\u25b0\u25b0, \u25b0\u25b03\u25b0\u25b0, epochs=\u25b0\u25b04\u25b0\u25b0, validation_data = (\u25b0\u25b05\u25b0\u25b0, \u25b0\u25b06\u25b0\u25b0))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 8:\n", "<div class=\"alert alert-block alert-info\">  \n", "For some Neural Networks, 100 epochs is more than ample time to reach a best solution. For others, 100 epochs isn't enough time for the learning process to even get underway. One good method for assessing the progress of your model at a glance involves visualizing how your loss scores and metric(s) -- for both your training and validation sets) -- changed during training. \n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "After 100 epochs of training, is the model still appreciably improving? (If it is still improving, you shouldn't see much evidence of overfitting). Submit your answer as a boolean value (True = still improving, False = not still improving). \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["", "pd.DataFrame(history.history).plot(figsize = (8, 8))\n", "plt.grid(True)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 9:\n", "<div class=\"alert alert-block alert-info\">  \n", "Regardless of whether this model is done or not, it's time to dig into what our model has done. Here, we'll continue re-tracing the steps taken in the textbook, producing a (considerably more involved) confusion matrix, visualizing it as a heatmap, and peering into our model's soul. The first step in this process involves creating the confusion matrix.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Using the held-back test data, create a confusion matrix. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred = np.argmax(nn_model.predict(\u25b0\u25b01\u25b0\u25b0), axis=1)\n", "\n", "y_true = np.argmax(\u25b0\u25b02\u25b0\u25b0, axis=1)\n", "\n", "conf_mat = tf.math.confusion_matrix(\u25b0\u25b03\u25b0\u25b0, \u25b0\u25b04\u25b0\u25b0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 10:\n", "<div class=\"alert alert-block alert-info\">  \n", "Finally, we're ready to visualize the matrix we created above. Rather than asking you to recreate the baroque visualization code, we're going to skip straight to interpretation. \n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Plot the confusion matrix heatmap and examine it. Based on what you know about the dataset, should the sum of the values in a column (representing the number of observations from a country) be the same for each country? If so, submit the integer that each column adds up to. If not, submit 0.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["", "sns.set(rc={'figure.figsize':(12,12)})\n", "plt.figure()\n", "sns.heatmap(\n", "    np.array(conf_mat).T,\n", "    xticklabels=country_encoder.classes_,\n", "    yticklabels=country_encoder.classes_,\n", "    square=True,\n", "    annot=True,\n", "    fmt='g',\n", ")\n", "plt.xlabel(\"Observed\")\n", "plt.ylabel(\"Predicted\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 11:\n", "<div class=\"alert alert-block alert-success\">\n", "Based on what you know about the dataset, should the sum of the values in a row (representing the number of observations your model <b>predicted</b> as being from a country) be the same for each country? If so, submit the integer that each row adds up to. If not, submit 0.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [""]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 12:\n", "<div class=\"alert alert-block alert-success\">\n", "If your model was built and run to the specifications outlined in the assignment, your results should include at least three countries whose observations the model struggled to identify (fewer than 7 accurate predictions each). Submit the name of one such country.<br><br>As a result of the randomness inherent to these models, it is possible that your interpretation will be correct, but will be graded as incorrect. If you feel that your interpretation was erroneously graded, please email a screenshot of your confusion matrix heatmap to Pierson along with an explanation of how you arrived at the answer you did. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [""]}], "metadata": {"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": false, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}, "varInspector": {"cols": {"lenName": 16, "lenType": 16, "lenVar": 40}, "kernels_config": {"python": {"delete_cmd_postfix": "", "delete_cmd_prefix": "del ", "library": "var_list.py", "varRefreshCmd": "print(var_dic_list())"}, "r": {"delete_cmd_postfix": ") ", "delete_cmd_prefix": "rm(", "library": "var_list.r", "varRefreshCmd": "cat(var_dic_list()) "}}, "types_to_exclude": ["module", "function", "builtin_function_or_method", "instance", "_Feature"], "window_display": false}}}, "nbformat": 4, "nbformat_minor": 4}