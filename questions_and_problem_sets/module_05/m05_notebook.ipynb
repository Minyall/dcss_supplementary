{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<br><br><font color=\"gray\">DOING COMPUTATIONAL SOCIAL SCIENCE<br>MODULE 5 <strong>PROBLEM SETS</strong></font>\n", "\n", "# <font color=\"#49699E\" size=40>MODULE 5 </font>\n", "\n", "\n", "# What You Need to Know Before Getting Started\n", "\n", "- **Every notebook assignment has an accompanying quiz**. Your work in each notebook assignment will serve as the basis for your quiz answers.\n", "- **You can consult any resources you want when completing these exercises and problems**. Just as it is in the \"real world:\" if you can't figure out how to do something, look it up. My recommendation is that you check the relevant parts of the assigned reading or search for inspiration on [https://stackoverflow.com](https://stackoverflow.com).\n", "- **Each problem is worth 1 point**. All problems are equally weighted.\n", "- **The information you need for each problem set is provided in the blue and green cells.** General instructions / the problem set preamble are in the blue cells, and instructions for specific problems are in the green cells. **You have to execute all of the code in the problem set, but you are only responsible for entering code into the code cells that immediately follow a green cell**. You will also recognize those cells because they will be incomplete. You need to replace each blank `\u25b0\u25b0#\u25b0\u25b0` with the code that will make the cell execute properly (where # is a sequentially-increasing integer, one for each blank).\n", "- Most modules will contain at least one question that requires you to load data from disk; **it is up to you to locate the data, place it in an appropriate directory on your local machine, and replace any instances of the `PATH_TO_DATA` variable with a path to the directory containing the relevant data**.\n", "- **The comments in the problem cells contain clues indicating what the following line of code is supposed to do.** Use these comments as a guide when filling in the blanks. \n", "- **You can ask for help**. \n", "\n", "Finally, remember that you do not need to \"master\" this content before moving on to other course materials, as what is introduced here is reinforced throughout the rest of the course. You will have plenty of time to practice and cement your new knowledge and skills.", "\n", "<div class='alert alert-block alert-danger'>", "As you complete this assignment, you may encounter variables that can be assigned a wide variety of different names. Rather than forcing you to employ a particular convention, we leave the naming of these variables up to you. During the quiz, submit an answer of 'USER_DEFINED' (without the quotation marks) to fill in any blank that you assigned an arbitrary name to. In most circumstances, this will occur due to the presence of a local iterator in a for-loop.", "</b>", "</div>"]}, {"cell_type": "markdown", "id": "standard-kennedy", "metadata": {}, "source": ["## Package Imports"]}, {"cell_type": "code", "execution_count": 1, "id": "precise-yahoo", "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "from pprint import pprint\n", "\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.decomposition import PCA\n", "from sklearn.cluster import KMeans\n", "from sklearn.metrics import silhouette_score, silhouette_samples\n", "\n", "\n", "\n", "import matplotlib as mpl\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "%config Completer.use_jedi = False"]}, {"cell_type": "markdown", "id": "separated-greensboro", "metadata": {}, "source": ["## Defaults"]}, {"cell_type": "code", "execution_count": 2, "id": "special-appeal", "metadata": {}, "outputs": [], "source": ["seed = 7"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 1:\n", "<div class=\"alert alert-block alert-info\">  \n", "Let's dig into our subsetted data. Notice that many of the variables have a range (min to max) of 0 to 1. In the next exercises we'll be making some comparisons between variables, so let's simplify those comparisons by selecting only the meta-data columns and columns with a range of 0 to 1.\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "Create a new list to filter the dataframe columns, initializing it with the names of the three meta-data columns. Append that list with the names of columns from vd_index_vars, only if the data in those columns falls within the required range. Subset the dataframe using this updated list. Fill in the blanks to continue.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(PATH_TO_DATA/'vdem_subset.csv', low_memory=False, index_col=0)\n", "\n", "vd_meta_vars = ['country_name', 'year', 'e_regiongeo']\n", "vd_index_vars = ['v2x_freexp_altinf', 'v2x_frassoc_thick', 'v2x_suffr', 'v2xel_frefair', 'v2x_elecoff',    # electoral democracy index\n", "              'v2xcl_rol', 'v2x_jucon', 'v2xlg_legcon',                                                 # liberal democracy index\n", "              'v2x_cspart', 'v2xdd_dd', 'v2xel_locelec', 'v2xel_regelec', 'v2x_polyarchy',              # participatory democracy index\n", "              'v2dlreason', 'v2dlcommon', 'v2dlcountr', 'v2dlconslt', 'v2dlengage',                     # deliberative democracy index\n", "              'v2xeg_eqprotec', 'v2xeg_eqaccess', 'v2xeg_eqdr']   \n", "sdf = df[vd_meta_vars + vd_index_vars]\n", "\n", "\n", "sub_vd_indices = []\n", "# iterate over the columns of vd_index_vars\n", "\u25b0\u25b01\u25b0\u25b0 column in vd_index_vars:\n", "    # filter out columns that have values greater than 1 or less than 0\n", "    \u25b0\u25b02\u25b0\u25b0 sdf[column].min() >= 0 \u25b0\u25b03\u25b0\u25b0 sdf[column].max() <= 1:\n", "        # add columns that pass the filter to a list of such columns\n", "        sub_vd_indices.append(\u25b0\u25b04\u25b0\u25b0)\n", "\n", "# create a new dataframe consisting of only those columns saved in sub_vd_indices and vd_meta_vars\n", "fsdf = sdf[vd_meta_vars \u25b0\u25b05\u25b0\u25b0 sub_vd_indices]\n", "\n", "fsdf.describe()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 2:\n", "\n", "<div class=\"alert alert-block alert-info\">\n", "In this problem set, we will continue to compute some descriptive statistics for our subsetted data and create some visualizations. We need a list that has the column names for just the variables, so we can re-use the 'subset_vd_indices' list from the previous problem. This code block will modify our dataframe so that it's easy to generate a single plot with all of the variables.\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "Use the <code>fsdf_ecdf</code> dataframe to create an empirical cumulative distribution plot of the indicator variables, using hue to differentiate them.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fsdf_ecdf = fsdf[sub_vd_indices]\n", "fsdf_ecdf = fsdf_ecdf.melt(value_vars = sub_vd_indices, var_name = 'vd_index', value_name = 'score')\n", "\n", "# create a new matplotlib figure\n", "figure = \u25b0\u25b01\u25b0\u25b0.figure(figsize=(10,6))\n", "# use seaborn to create the plot\n", "ax = sns.\u25b0\u25b02\u25b0\u25b0(fsdf_ecdf, x = '\u25b0\u25b03\u25b0\u25b0', hue = '\u25b0\u25b04\u25b0\u25b0', kind = \"ecdf\")\n", "ax.set(xlabel='Score', ylabel='Proportion', title=\"ECDF for VDEM Indicator Variables\")\n", "figure.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 3:\n", "<div class=\"alert alert-block alert-info\">\n", "We can see that most of the variables follow a fairly smooth curve, while a few see dramatic proportion increases in a step-like way. This indicates that although we might consider these to be continuous variables, the measurement that produced them had some discrete (interval-like) qualities.\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "Select one variable that seems to have a relatively smooth distribution and another that has a distinctly step-like distribution. Fill their names into the underscore blanks (<code>__A__</code> and <code>__B__</code>) in the 'x' and 'y' variables in the `sns.distplot` call in the code block below. There are a number of options to choose from for each, so the distinction between the two is what matters. Produce a bivariate kernel density estimation rug plot for the two selected variables. Fill in the blanks to continue.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# create a new matplotlib figure\n", "figure = \u25b0\u25b01\u25b0\u25b0.figure()\n", "# use seaborn to create the plot\n", "ax = sns.\u25b0\u25b02\u25b0\u25b0(fsdf, x=__A__, y=__B__, kind=\"\u25b0\u25b03\u25b0\u25b0\", rug = \u25b0\u25b04\u25b0\u25b0, rug_kws = {\"alpha\": 0.01})\n", "sns.despine()\n", "ax.set(xlabel='frassoc_thick', ylabel='polyarchy')\n", "figure.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 4:\n", "\n", "<div class=\"alert alert-block alert-info\">\n", "Now we'll create a correlation matrix (2D array) of all the variables and plot it in a heatmap to see which pairs of variables are most and least correlated. \n", "<br><br>\n", "We'll use a boolean mask to clean-up the heatmap. Remember that a boolean mask is an array of \"True\" and \"False\" values, the same size and shape as the data array, where a value of \"False\" indicates that the value in the data array should be ignored. In this case, the mask will remove all values on the top-left -> bottom-right diagonal and above that diagonal.\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "Create a heatmap of the correlation matrix. Use the heatmap to select a few correlations to print. \n", "Looking at the heatmap above, select 2 pairs of variables (4 variables total) that appear highly correlated with each other. Then, select another 2 pairs of variables (4 variables total) that appear minimally correlated with each other. Create two lists, one for the first element of each variable pair and one for the second element of each pair. These lists should be aligned. Replace the underscore blanks (<code>__A__</code> and <code>__B__</code>) with your aligned lists of variables. Jointly iterate over the two lists and print the resulting Pearson correlations between each variable pair. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fsdf_corr = fsdf[sub_vd_indices].corr()\n", "# create the upper triangular mask\n", "mask = np.\u25b0\u25b01\u25b0\u25b0(np.\u25b0\u25b02\u25b0\u25b0(fsdf_corr, dtype = bool))\n", "figure = plt.figure()\n", "# create the masked heatmap\n", "ax = sns.\u25b0\u25b03\u25b0\u25b0(data = fsdf_corr, \u25b0\u25b04\u25b0\u25b0 = \u25b0\u25b05\u25b0\u25b0)\n", "figure.show()\n", "\n", "var_1_list = __A__\n", "var_2_list = __B__\n", "\n", "for v1, v2 in \u25b0\u25b06\u25b0\u25b0(var_1_list, var_2_list):\n", "    result = \u25b0\u25b07\u25b0\u25b0[v1].\u25b0\u25b08\u25b0\u25b0(\u25b0\u25b09\u25b0\u25b0[v2])\n", "    print('Correlation of ' + v1 + ' and ' + v2 + ' : ' + str(\u25b0\u25b010\u25b0\u25b0))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 5:\n", "<div class=\"alert alert-block alert-info\">  \n", "For the remainder of the assignment, we're going to be working with data from the European Values Survey (EVS). The data is comprised of data collected by interviewers, and was drawn from most European nations in 2017. We'll start by loading a subset of the EVS and then standardizing each of the variables. Standardized data is a <i>sine qua non</i> when working with latent variables!\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Standardize the data present in the columns of the <code>evs_df</code> dataframe. To accomplish this, use the <code>StandardScaler</code> class from the scikit-learn package. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["evs_df = pd.read_csv(PATH_TO_DATA/\"evs_subset.csv\")\n", "\n", "country_index = evs_df['country'].to_numpy()\n", "\n", "# Drop the country column, as it cannot be standardized\n", "evs_df = evs_df.\u25b0\u25b01\u25b0\u25b0(\"country\", axis=\u25b0\u25b02\u25b0\u25b0)\n", "\n", "# Standardize the data\n", "X = \u25b0\u25b03\u25b0\u25b0().\u25b0\u25b04\u25b0\u25b0(evs_df)\n", "\n", "X"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 6:\n", "<div class=\"alert alert-block alert-info\">\n", "Even though our input to the StandardScaler in the previous question was a dataframe, you may have noticed that the resulting output was an Numpy array. Fortunately, Pandas and Numpy have been built to interoperate with one another smoothly. Pandas Dataframes and multidimensional Numpy arrays can be interoperable (although with a different set of features); the same goes for Pandas Series and unidimensional Numpy arrays. Let's return to the 'evs_df' dataframe we made in the previous problem; it will be useful for exploring how numpy handles multidimensional arrays.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "In this exercise, we're going to turn a subset of our Pandas dataframe (including only variables v145, v146, v147, and v148) into a multidimensional Numpy array, and convert every number it contains into a whole-number percentage (which we can accomplish by multiplying by dividing by 5, multiplying by 100, and rounding to the nearest whole number). Fill in the blanks to continue.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Convert the numerical columns of our dataframe to array format \n", "evs_array = \u25b0\u25b01\u25b0\u25b0(evs_df[['v145', 'v146', 'v147', 'v148']])\n", "\n", "# Divide every value in the array by 5, then multiply by 100\n", "evs_array_percent = (evs_array \u25b0\u25b02\u25b0\u25b0 5) \u25b0\u25b03\u25b0\u25b0 100 \n", "\n", "# Round each value in the array to the nearest whole number\n", "evs_array_percent_r = np.\u25b0\u25b04\u25b0\u25b0(evs_array_percent)\n", "\n", "evs_array_percent_r"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 7:\n", "<div class=\"alert alert-block alert-info\">\n", "We can also use Numpy to rapidly and simply perform linear algebra calculations. If, for example, we wanted to see how 'v145' (which measures respondents' preference for a 'strong leader') and 'v148' (which measures respondents' preference for democratic norms) covary with one another, we can produce a covariance matrix. Instead of using the percentage-valued array we created in the previous problem, we'll instead use a standardized array, which will make the covariance matrix a little easier to read.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "In the code cell below, use Numpy to extract the 'v145' and 'v148' variables. Then, use <code>np.cov</code> to create a covariance matrix between the <code>strong_leader</code> and <code>democratic</code> variables. Fill in the blanks to continue. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Isolate the 'Strong Leader' column from the array (first column)\n", "strong_leader = evs_array[:,\u25b0\u25b01\u25b0\u25b0]\n", "# Isolate the 'Democratic' column from the array (last column)\n", "democratic = evs_array[:,\u25b0\u25b02\u25b0\u25b0]\n", "\n", "# Compute the covariance of the two\n", "np.\u25b0\u25b03\u25b0\u25b0(strong_leader, democratic)\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 8:\n", "<div class=\"alert alert-block alert-info\">  \n", "We'll proceed with by using our now-standardized data as the basis for a principal components analysis. Unlike in the chapter, however, we're only going to have our PCA return the top 10 components.\n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Perform a principal components analysis on the standardizd EVS data. Only return the top 10 components (sorted in order of explained variance ratio). Submit a numpy array containing the explained variance ratios of the 10 principal components you found. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create PCA with 10 components\n", "pca = \u25b0\u25b01\u25b0\u25b0(\u25b0\u25b02\u25b0\u25b0, random_state=42)\n", "\n", "# Fit the PCA\n", "pca.\u25b0\u25b03\u25b0\u25b0(\u25b0\u25b04\u25b0\u25b0)\n", "\n", "# Extract explained variance ratio\n", "evr = pca.\u25b0\u25b05\u25b0\u25b0\n", "\n", "print(evr)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem 9:\n", "<div class=\"alert alert-block alert-info\">  \n", "Although the process of interpreting screeplots is generally subjective and open to interpretation, we're fortunate in that the screeplot from our PCA of the EVS data has a clear inflection point. It's time to flex your interpretation muscles! \n", "</div>\n", "<div class=\"alert alert-block alert-success\">\n", "Produce a screeplot of the 10 principal components you produced in question 2. Submit an integer corresponding to the principal component ID that corresponds with the inflection point.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["", "\n", "# Extract explained variances\n", "eigenvalues = pd.Series(pca.explained_variance_)\n", "\n", "# Create screeplot\n", "fig, ax = plt.subplots()\n", "sns.lineplot(x=eigenvalues.index, y=eigenvalues, data=eigenvalues)\n", "plt.scatter(x=eigenvalues.index, y=eigenvalues)\n", "ax.set(xlabel='Principal component ID', ylabel='Eigenvalue')\n", "sns.despine()\n", "plt.show()"]}], "metadata": {"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": false, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}, "varInspector": {"cols": {"lenName": 16, "lenType": 16, "lenVar": 40}, "kernels_config": {"python": {"delete_cmd_postfix": "", "delete_cmd_prefix": "del ", "library": "var_list.py", "varRefreshCmd": "print(var_dic_list())"}, "r": {"delete_cmd_postfix": ") ", "delete_cmd_prefix": "rm(", "library": "var_list.r", "varRefreshCmd": "cat(var_dic_list()) "}}, "types_to_exclude": ["module", "function", "builtin_function_or_method", "instance", "_Feature"], "window_display": false}}}, "nbformat": 4, "nbformat_minor": 4}
